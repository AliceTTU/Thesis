
\documentclass[12pt]{article}     
\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage[top=2.5cm, bottom=2.5cm, left=3cm, right=3cm]{geometry}
\usepackage{titlesec}
\usepackage{longtable}
\usepackage[table,xcdraw]{xcolor}
\usepackage{todonotes}
\usepackage{float}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{tocloft}
\usepackage{amssymb}
\renewcommand{\labelitemi}{\tiny$\blacksquare$} %For square itemized lists
\usepackage{caption} 
\captionsetup{labelsep=period}
\usepackage{verbatimbox} %To put program code in the center using Verbatim
\titlelabel{\thetitle.\quad}
\usepackage{times}
\usepackage{fancyhdr}
\setlength{\parindent}{0cm}
\usepackage{setspace}
\onehalfspacing
\setlength{\parskip}{\baselineskip}
\usepackage{amsmath} 
\usepackage{amsthm}
% Packages for building tables and tabulars 
\usepackage{array}
\usepackage{tabu}   % Wide lines in tables
\usepackage{xspace} % Non-eatable spaces in macros
\usepackage[colorlinks=true,linkcolor=blue]{hyperref}
\usepackage[all]{hypcap}
\usepackage{url}
% Packages for defining colourful text together with some colours
%\usepackage[table,xcdraw]{xcolor}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\definecolor{lightblue}{rgb}{0.95,0.97,1.0}
\definecolor{darkblue}{rgb}{0.90,0.92,1.0}
\usepackage{color}

% Standard package for drawing algorithms
% Since the thesis in article format we must define \chapter for
% the package algorithm2e (otherwise obscure errors occur) 
\let\chapter\section
\usepackage{algorithm2e}

% Macros that make sure that the math mode is set
\newcommand{\typeF}[1] {\ensuremath{\mathsf{type_{#1}}}\xspace}
\newcommand{\opDiv}{\ensuremath{\backslash \mathsf{div}}\xspace} 
\usepackage{listings}

\lstset{ 
  %language=python,                % the language of the code
  language=C++,
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  %numbers=left,                   % where to put the line-numbers
  %numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
  numberstyle=\tiny\color{gray}, 
  stepnumber=1,                    % the step between two line-numbers. If it's 1, each line 
                                   % will be numbered
  numbersep=5pt,                   % how far the line-numbers are from the code
  backgroundcolor=\color{white},   % choose the background color. You must add \usepackage{color}
  showspaces=false,                % show spaces adding particular underscores
  showstringspaces=false,          % underline spaces within strings
  showtabs=false,                  % show tabs within strings adding particular underscores
  frame = lines,
  %frame=single,                   % adds a frame around the code
  rulecolor=\color{black},		   % if not set, the frame-color may be changed on line-breaks within 
                                   % not-black text (e.g. commens (green here))
  tabsize=2,                       % sets default tabsize to 2 spaces
  captionpos=b,                    % sets the caption-position to bottom
  breaklines=true,                 % sets automatic line breaking
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  %title=\lstname,                 % show the filename of files included with \lstinputlisting;
                                   % also try caption instead of title
                                   % also try caption instead of title
  keywordstyle=\color{blue},       % keyword style
  commentstyle=\color{dkgreen},    % comment style
  stringstyle=\color{mauve},       % string literal style
  escapeinside={\%*}{*)},          % if you want to add a comment within your code
  morekeywords={*,game, fun}       % if you want to add more keywords to the set
}

\usepackage{multirow}
\setlength{\tabcolsep}{0pt}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1-\arrayrulewidth\relax}}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1-\arrayrulewidth\relax}}

\usepackage{booktabs,fixltx2e}
\usepackage{tikz}
%used for ex. for m prime
\usepackage{flexisym}
\usepackage{footnote}

\usepackage{arydshln}
\usepackage{placeins}
\usepackage{enumitem}
\usepackage{cleveref}
\crefformat{footnote}{#2\footnotemark[#1]#3}
\usepackage{IEEEtrantools}

\begin{document}
\bstctlcite{IEEEexample:BSTcontrol}
%------------------------------TIITELLEHT---------------------------------
\thispagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\headheight = 57pt
\footskip = 11pt
\headsep = 0pt

\chead{
 \textsc{\begin{Large} %Tekst suurtähtedega ja suuremaks
	Tallinn University of Technology\\
	\end{Large} }
	Department of Computer Science\\	
	TUT Center for Digital Forensics and Cyber Security
}
\vspace*{7 cm}

\begin{center}
ITC70LT\\[0cm]

Gvantsa Grigolia 144965\\
\vspace{15pt}
\begin{LARGE}
\textsc{Evaluation of data ownership solutions in remote storage.\\}
\end{LARGE}
\vspace{10pt}
Master Thesis\\[2cm]
\end{center}

\begin{flushright}
Supervisor: Ahto Buldas\\[0cm]
Professor \\[0cm]
\end{flushright}

\cfoot{Tallinn 2016} 
\pagebreak

%---------------------------AUTORIDEKLARATSIOON-------------------------
\section*{\begin{center}
 Autorideklaratsioon
\end{center}}


Autorideklaratsioon on iga lõputöö kohustuslik osa, mis järgneb tiitellehele.
Autorideklaratsioon esitatakse järgmise tekstina:

Olen koostanud antud töö iseseisvalt. Kõik töö koostamisel kasutatud teiste autorite tööd, olulised seisukohad, kirjandusallikatest ja mujalt pärinevad andmed on viidatud. Käsolevat tööd ei ole varem esitatud kaitsmisele kusagil mujal.

Autor: [Ees$-$ ja perenimi]

[\today]
\pagebreak

%---------------------------ANNOTATION---------------------------------
\section*{\begin{center}
Annotatsioon
\end{center}}

Annotatsioon on lõputöö kohustuslik osa, mis annab lugejale ülevaate töö eesmärkidest, olulisematest käsitletud probleemidest ning tähtsamatest tulemustest ja järeldustest. Annotatsioon on töö lühitutvustus, mis ei selgita ega põhjenda midagi, küll aga kajastab piisavalt töö sisu. Inglisekeelset annotatsiooni nimetatakse Abstract, venekeelset aga


Sõltuvalt töö põhikeelest, esitatakse töös järgmised annotatsioonid:
\begin{itemize}
\item kui töö põhikeel on eesti keel, siis esitatakse annotatsioon eesti keeles mahuga $\frac{1}{2	}$ A4 lehekülge ja annotatsioon \textit{Abstract} inglise keeles mahuga vähemalt 1 A4 lehekülg;
\item kui töö põhikeel on inglise keel, siis esitatakse annotatsioon (Abstract)  inglise keeles mahuga $\frac{1}{2}$ A4 lehekülge ja annotatsioon eesti keeles mahuga vähemalt 1 A4 lehekülg;
\end{itemize}

Annotatsiooni viimane lõik on kohustuslik ja omab järgmist sõnastust:

Lõputöö on kirjutatud [mis keeles] keeles ning sisaldab teksti [lehekülgede arv] leheküljel, [peatükkide arv] peatükki, [jooniste arv] joonist, [tabelite arv] tabelit.
\pagebreak


%-----------------------------ABSTRACT-----------------------------------

\section*{\begin{center}
Abstract
\end{center}}
Võõrkeelse annotatsiooni koostamise ja vormistamise tingimused on esitatud eestikeelse annotatsiooni juures.

The thesis is in [language] and contains [pages] pages of text, [chapters] chapters, [figures] figures, [tables] tables.
\pagebreak

%---------------------Glossary of terms and Abbreviations---------------------

%\section*{\begin{center}
%Glossary of Terms and Abbreviations
%\end{center}}
%Lühendite  ning  mõistete  sõnastikku  lisatakse kõik töö põhitekstis kasutatud  uued  ning  ka mitmetähenduslikud üldtuntud terminid. Näiteks inglisekeelne lühend PC  võib tähendada nii Personal Computer kui ka Program Counter, sõltuvalt kontekstist. Lühendid ja mõisted esitatakse tabuleeritult kahte tulpa selliselt, et vasakul on esitatud lühend või mõiste ja paremal tulbas seletus. Inglisekeelsed sõnad seletustes esitatakse kaldkirjas. Alltoodud näited esitavad lühendite ja mõistete sõnastiku korrektset vormistamist.

%\begin{tabular}{p{3 cm}ll}
%IPv6&Internet Protocol version 6\\
%ICMPv6&Internet Control Message Protocol version 6\\
%Node&ll\\
%NAT&dd\\
%IANA&Internet Assigned Numbers Authority\\
%BYID&Bring Your Own Device\\
%OS&Operating System\\
%IoT&Internet of Things\\
%rootkit&ff
%\end{tabular}
%\pagebreak

\tableofcontents
\newpage
\listoffigures
\pagebreak
\listoftables
\pagebreak

\section{Introduction}
\label{sec:1}

Describes the problem statement, illustrates why this is a problem and describes the contribution the thesis makes in solving this problem. Optionally, 
it can give a short description (1-3 sentences each) of the remaining chapters. Good introductions are concise, typically no longer than 4 pages.\\
%The introduction reveals the full (but summarized) results of your work. This appears counter-intuitive: does this not break the tension, 
%like revealing the name of the murderer on the first page of a thriller? Yes, it does. That is the whole point. A thesis, and thus its architecture, 
%aims primarily to inform, not entertain.

\pagebreak
%---------------------------------TERMS AND DEFINITION -----------------------------------------
\section{Terms and definition}
\label{sec:2}
Defines the fundamental concepts your thesis builds on. Your thesis implements a new type of parser generator and uses the term non-terminal symbol a lot? 
Here is where you define what you mean by it. The key to this chapter is to keep it very, very short. 
Whenever you can, don’t reinvent a description for an established concept, but reference a text book or paper instead.
\pagebreak
%------------------------------BACKGROUND AND RELATED WORK-----------------------------------
\section{Background and Related Work}
\label{sec:3}
%4 -5 pages 
\subsection{Data Deduplication}
\label{sub:Deduplication}
Cloud computing is an on-demand service. Customers are charged based on used storage and bandwidth.\footnote{"With Amazon S3, you pay only for the storage you actually use. There is no minimum fee and no setup cost. Amazon S3 has three pricing components: storage (per GB per month), data transfer in or out (per GB per month), and requests (per n thousand requests per month)." \url{http://aws.amazon.com/s3/pricing/}} Both service providers and customers are interested in cost efficient solutions of cloud storage. Data deduplication offers disk and bandwidth savings. Idea is simple -- avoid or remove a duplicated data. 
This section covers basic concepts of deduplication technology. Lists various methods and processing type sand underlines the approaches used in cloud storage.

\subsubsection{Hash Based Deduplication}
\label{subsub:HashBased}
To remove or avoid duplicated data, it must be detected first. Hash based data deduplication uses the hash values of a file (or data chunk) as a file (or data chunk) identifier. Hashes of files are calculated and then are kept on the server. When the file is uploaded first time, its hash is computed and it is compared with the existing hashes on the server. If there is a match, the file is not stored on the disk (or in case of client-side deduplication, is not transfered at all). Instead, server creates the reference, which points on the already existing file, with the same hash value. If computed hash does not match with any of the hashes, the file together with the hash value is stored on the server.\cite{DeDupOverView}

\subsubsection{Types of Deduplication}
\label{subsub:TypesOfDedup}
Data deduplication differs based on processing methods. If it takes place before the client application transfers the file to the server, it is known as client-side deduplication. If it takes place, after the file is uploaded on server, it is known as server side deduplication. In client-side deduplication scenario, the client application computes the hash of the file and sends it to the server. If the hash already exists on the server side, client application does not send the file. If no match is found, it means,that the file is unique and client application transfers it to the server. On the other hand, if client application directly sends file to the server and server computes the hash after it, it is called server-side deduplication. Both processing methods save storage, but client-side deduplication also reduces bandwidth consumption.\cite{DeDupMethods}\\\\
Apart of divers processing methods, data deduplication differs in processing levels. There are file  and block level data deduplication. Difference between them is intuitive. In case of file level, hash of file is calculated and as a result server stores unique files. In block level scenario, files are divided into blocks(fixed or variable size). Hashes of these blocks are calculated and duplicated data on block level is avoided. \cite{DeDupMethods}\\\\
The last concept is, single and cross client data deduplication. Single client data deduplication removes duplicated data  in scope of one user. Duplicated data will be stored on the server, if it belongs to different users. On the other hand, cross client deduplication vanishes the user boundaries and unique data would be shared among the users.\cite{DeDupMethods}

\subsubsection{Summary}
\label{subsub:SummaryDedup}
Cloud storage providers are looking for, the most efficient way to reduce the cost. In cross user client-side deduplication  case, file or ”chunks” of file are stored only ones on the disk and users are sharing the data. It reduces the bandwidth cost dramatically, because the deduplication takes palace on client side, and duplicated files are not uploaded at all.\cite{DeDupDropBox} Such cost reductions is attractive for cloud storage providers, but this technology has some security drawbacks. \hyperref[sub:ConfidentialityAndPrivacy]{Next section} covers potential attacks taking place during cross-user client-side data deduplication in cloud storage.


\subsection{Confidentiality and Privacy Issues in Remote Storage}
\label{sub:ConfidentialityAndPrivacy}
%$f$
Although deduplication is a beneficial technology, there are security drawbacks, leading to potential attacks. Taking into consideration the behavior of the cross-user client-side data deduplication, it is easy to learn some general facts. This section focuses on attacks breaching the confidentiality of data and privacy of remote storage customers, when cross-user client-side data deduplication takes place.

\subsubsection{Potential Attacks}
\label{subsub:TypesOfDedup}
Danny Harnik was first who has demonstrated, the potential attacks in remote storage related to data deduplication technology.\cite{Harnik} The paper covers three cases: file detection, file content detection and covert channel. The first case shows, how  trivial is  to learn whether the remote server already contains the particular file or not. Attacker uploads the file and observes the network traffic or the time required to upload the file. If the file already is stored on the server, there is no need to upload it again. Client application sends only the 
hash of the file to the server. The observer detects, that amount of data is smaller then file's size itself(Size of the hash depends on hash function and is smaller then file size). If file is "big enough", observing the time required to upload file on server, is sufficient to learn, whether the server already contains the file or not. The law enforcement authorities, can use this behavior. Check if storage provider contains the file (e.g. file's content is against the law) and later, they can force remote storage service providers to revile the identity of the file owner.\\\\
Data deduplication technology opens the possibility to guess the content of user's data.The approach is straightforward, attacker just tries all possible variations and waits for occurrence of data deduplication. Once it takes place, attacker learns that such file (file with this content) exist on the server. The trick is that, unlike the dictionary attacks it is not detectable. It is the legitimate way to upload new documents on the server.\cite{Harnik} This type of attack is easy to lunch against the files with small min-entropy. To have batter understanding, refer to the following example. Bob is invited at the event in the cinema. He stores his invitation ticket in the cloud. Alice wants to learn the row and the place of Bob's ticket. She put the Bob's name on the right place and  starts to brute force row and place numbers. Alice generates files with different content and uploads on cloud. Once the deduplication takes place, she will get the desired information.\\\\
Last case describes the covert channel. Precondition for this scenario is, that attacker already have to own the victims machine. In order to exchange one bit information "0" or "1", attacker generates two random files  and uploads one of them. If the first file is uploaded the covert channel transfers "0" else it transfers "1" bit. Covert channel can transfers more information, by altering  the number of files or the meaning of file.\cite{Harnik}\\\\
All above stated attacks demonstrates the side channel effects of data deduplication. Attackers exploit the vulnerability, that data deduplication is detectable. But later Halevi states that main issue is not the detectability, but using the hash value as a proxy in remote storage.\cite{PoW} He claims that, to use a hash as a proxy to retrieve the file is vulnerable. Owning a small static piece of the file(e.g. hash of the file) does not necessarily mean owning the entire file. He referees to the Dropship\footnote{\url{https://github.com/driverdan/dropship} - "Instantly transfer files between Dropbox accounts using only their hashes"} open source project, as a brief example of misusing the storage provider. Dropship turn the remote storage  provider into CDN (Content Distribution Network) service. For that time Dropbox\footnote{\url{https://www.dropbox.com/}} was operating based on the cross-user client-side deduplication. The users of Dropship, where able to download the file in their folder, just sending the file’s hash for check to the Server. This open source project was considered as the violation of Terms of Service of the company and is not operating anymore. Halevi introduces the Proof of Ownership Protocol, which dramatically reduces the probability of the attacker to retrieve the file, without owning it. \hyperref[sec:4]{Next section} covers the detail description of Proof of Ownership Protocol and other solutions offered to substitute the hash as a proxy approach for data ownership in remote storage.\cite{PoW}
%Collects descriptions of existing work that is related to your work. Related, in this sense, means aims to solve the same problem or uses the same approach to solve a different problem. 
%This chapter typically reads like a structured list. Each list item summarizes a piece of work (typically a research paper) briefly and explains the relation to your work. 
%This last part is absolutely crucial: the reader should not have to figure out the relation himself.
%Is your piece better from some perspective? More generalizable? More performant? Simpler? It is ok if it is not, but I want you to tell me.

\subsection{Summary}
\label{subsub:AttackSummary}
The amount of savings offered by data deduplication, depends on data type and content produced by users of such services.\cite{ratio}  In case of office workers as users in remote storage, the benefit from deduplication is high. Office workers use mostly identical template to generate the data and the portion of duplication is high. Applying data deduplication technology saves bandwidth and disk space.  But same time it rises privacy and confidentiality issues. The major weakness is that, client-side deduplication is detectable and using hash as a proof of ownership is not sufficient. Anyone who possesses the hash value of file, is able to retrieve the file from the server.  If the attacker  obtains the hash of the file, he can retrieve the file from the server and gain unauthorized access to it.\\\\
Data privacy issue in cloud computing is one of the aspects that could break the trust of the users towards the service providers. So those who what to stay on the market, should build the systems, which takes into consideration privacy and confidentiality.

\pagebreak

%------------------------------APPROACH-----------------------------------
\section{Approach}
\label{sec:4}

We have demonstrated importance of data deduplication technology for remote storage services. And have determine the root  cause of breaching the privacy and confidentiality. This section covers the solutions, which refuse to use the static piece of information (hash of the file) as a proxy and offers alternative ways to proof the ownership of the data. We numerate the solutions from one to seven based on published date and show how it works and what are there security and efficiency characteristics. \\\\
\subsection{Solution \# 1}
\label{sub:Soltuion1}

This subsections covers Proof of Ownership (PoW) protocol, introduced by Halevi.\cite{PoW}PoW involves two parties: Prover and Verifier. The goal of prover is to convince the verifier, that he "owns" particular file. While the goal of verifier is to check if the affirmation of the prover is true. To accomplish their tasks, verifier uses summary value of file, while  prover relies on  the file itself. Paper \cite{PoW} offers three solutions, and the subsection reviews all of them, but covers security and efficiency characteristics only  for the last one. Before we move to the solutions, we have to underline two constraints. First, attacker may have compliances which own the file, but the total number of bits that attacker can receive from them must be less then initial min-entropy\footnote{"The min entropy, in information theory, is the smallest of the Rényi family of entropies, corresponding to the most conservative way of measuring the unpredictability of a set of outcomes, as the negative logarithm of the probability of the most likely outcome." "A random variable $X$ has \textbf{min-entropy} $k$, denoted $H_\propto(X)=k$, if  $\underset{x}{max}Pr[X=x]=2^{-k}$  "\cite{MinEnt} } of file. And second, attacker can not interact with compliances during the proving phase.( e.g. case misusing the remote storage as CDN)
\subsubsection{Setup}
\label{subsub:setup1}
The most secure and less efficient  from suggested three solutions, uses erasure code.\footnote{"The basic premise of erasure coding goes as follows: Take a file and split into k pieces and
encode into n pieces. Now, any k pieces can be used to get back the file”} Form each 90\% of bits, it is possible to recover the whole file. After the file is encoded using erasure code, next step is to build the Merkle-tree\cite{Merkle} on the encoded file. The verifier(server) keeps the root value of the computed tree and the number of leaves. During the poof phase, verifier(server) asks the  prover(client)  for some number of leaves' values and their sibling paths. The verifier checks if all the provided sibling paths gives the valid Merkle-tree root value. Based on the outcome, server grants or does not grant the access to the file.\\\\
Computing erasure code requires access to the file and in case of large files (the files stored on the secondary storage) it raises communication complexity. To increase the efficiency of the protocol, erasure encoding is substituted with universal hashing\cite{Hash}. First the file is hashed and then the Merkle-tree is built on the hash. The hashing servers to reduce the size of file up to some predefined number of bits(max length 64MByte). The second solution is more efficient then first one, but it  weakens the security. Security requirement for first solution claims: attacker can not retrieve file from the server, if the min-entropy remained in file after attacker receives the bits form compliances, is bigger then security parameter. Erasure encoding  substitution with universal hashing, made changes in security requirement as well. For second solution, security requirement stress that, attacker can convince the verifier to grant access to the file, if it receives some $T$ bits from compliances, which can be less then min-entropy of the file. (e.g.64MByte)\\\\
Erasure code and universal hashing solutions, both considers that the input file is taken form an arbitrary distribution. On the other hand, the third solution claims that, in realistic scenarios, the attacker always has some information about file which he desires to extract.Thereof, it is resalable to relax the security requirement and define  it for particular class of distribution. Such relaxation of security requirement gives possibility to modify the protocol and make it more space efficient.  In particular instead of working with bit vectors, it is possible to divide file into blocks and operate over the blocks. There are three phases to prepare the input for Merkle-Tree: Initializing, reducing and mixing. First the $M$ bit size file is divided into $m$ blocks. In the initializing phase, $l$ blocks of buffer and IV (Initial Vector) are allocated. Next comes reduction phase, which is a liner mapping. It maps, original file's $m$ blocks to the allocated $l$ buffer blocks. Each block of the file is XORed in specific number in some locations. And locations are taken from IV, which is generated as SHA256(IV[i-1],File[i]). Where $i$ is the block number of the file and IV[0] is defined as SHA256-IV.\footnote{For SHA-256, the initial hash value, H(0), consists of the eight 32-bit words, in hex. These words were obtained by taking the first thirty-two bits of the fractional parts of the square roots of the first eight prime numbers.\url{https://tools.ietf.org/html/rfc4634\#section-6.2}}. The same operations take palace at mixing phase. But with one difference, instead of file blocks, buffer blocks are taken as input of XORing. 
\subsubsection{Security And Efficiency}
\label{subsub:secAndeff1}


To demonstrate the soundness of the last solution, it is better to view the file from attacker's perspective. Input file in this scenarios is not take form arbitrary distribution, but form some class of distribution. And it reasonable for real life scenarios, as attacker always know some peace of information(e.g. file format) about the file that he tries to retrieve. $M$-bit file with $k$ bits of min-entropy, can be represented from attackers perspective as $\vec{f}\leftarrow\vec{w}\cdot A +\vec{b}$, where $\vec{w} \in \{0,1\}^k$  and is chosen randomly, while $A\in\{0,1\}^{k\times M}$ and $\vec{b}\in\{0,1\}^M$ are chosen by attacker(based some knowledge that attacker has). Protocol uses hash function to prepare input for Merkle-tree, which is linear mapping, $h(\vec{f})=\vec{f}\cdot C = \vec{w}\cdot AC + \vec{b}C$.\cite{PoW} Important part in this linear mapping is that the linear code that is generated by AC matrix to have a large minimum distance. And it is possible to achieve as we are choosing matrix C for mapping. The theorem \#3 proved in the paper states that the last solution is the secure proof of ownership with soundness $\left( \frac{L-d+1}{L}\right)^t$ where $L$ is reduce buffer, $t$ is number of challenges on Markle-tree and $d$ is the minimum distance of the linear code generated by AC matrix.("For example, if the code has minimum distance$\geqslant \frac{l}{3}$ then we get soundness of ($\frac{2}{3}$)$^t$ .")\cite{PoW}\\\\
Time efficiency is one of the important features, that characterizes the protocol and influences decision weather to implement it or not. Halevi evaluates the performance of PoW protocol, and  compares it with non-secure  \hyperref [sub:Deduplication]{data-deduplication} and whole file transfer (without data-deduplication) implementations of remote storage. Overall time protocol requires, is decomposed in three parts: Client, Server and Network time.\footnote{"The measurements were performed on an Intel machine with Xeon X5570 CPU running at 2.93GHz. We implemented the protocol in C++ and used the SHA256 implementation from Crypto++"} Client time is calculated as the sum of the subtasks client performs and subtasks are: reading file from the disk, computing the SHA256 hash, going through reduction and mixing phases and  computing the Merkle-tree. Server time -- the time server needs to check Mekle-tree authentication signature. And Network time -- respectively the time necessary for data generated by prover and verifier to travel via network. Server and Network time consumption is negligible. (E.e. checking 20 sibling paths "costs" 0.6ms and data generated for transmission  is less then 20K. In case of 5Mbps network the overhead is 0.1 ms. All together the overhead of Server and Network is 0.7 ms). While the main pressure comes on client side. To  compare it with insecure implementation of \hyperref [sub:Deduplication]{data-deduplication}, PoW on client side adds reduction and mixing phases and Markle-tree calculation. As result of the tests, reduction phase adds less then 28\% time over insecure solution. Mixing phase and Mekle-tree calculation behavior depends on th size of the file. For small size files(less then 64MByte), the time up-growth is 200\% , but it stays  constant(1158ms) once the file grows above 64MByte. PoW is also  compare with the solutions to avoid deduplication and always send a whole file to the server. Protocol is observed in two setups: network with 5Mbps and 100Mbps. The results are following: PoW always consumes less time in 5Mbps then transferring the whole file. Once the file grows over the 1GByt PoW requires 1\%  time of the file transfer. In case of 100Mbps network, the protocol has lower bound for file size. For files larger then 64K, PoW consumes less time then solution without deduplication . And for files larger then 1GByt, it requires 4\% of time of the whole file transfer. 

\begin{savenotes}
\begin{table}[!htpb]
\centering
\addtolength{\tabcolsep}{3pt}
\begin{tabular}{|L{1cm}|L{4.5cm}|L{4.5cm}|}
\rowcolor{lightblue}
\hline
&Dedup Time  $= T_d$ &File Transfer Time $=T$ \\
\hline
PoW & $\color{red} 3.28T_d+0.7ms$\footnote{For files less then 64Mb: $T_d+0.28T_d+2T_d+0.7ms$}  and $\color{red} 1.28T_d+1.165s$\footnote{For files more then 64Mb: $T_d+0.28T_d+2T_d+0.7ms$ } 
& $\color{red} 0.1T$\footnote{In 5Mbps network and file size more then 1Gb}; $\color{red} 0.4T$ \footnote{In 100Mbps network and file size more then 1Gb}; 
$\color{red} > T$ \footnote{In 5Mpbs for any size of file and In 100Mbps for files larger then 64K}; \\
\hline

\end{tabular}
\caption{PoW 1Time Comparison}
\label{table:paramProbCodes}
\end{table}
\end{savenotes}



\subsection{Solution \# 2}
\label{sub:Soltuion2}

This subsection covers the solution proposed by Di Pietro\cite{DiPietro} in his paper "Boosting Efficiency and Security in Proof of Ownership for Deduplication". The motivation of this work is to improve the efficiency of PoW\cite{PoW} protocol and avoid the security  assumptions that is hard to verify(refereeing to the concept that the file is taken from some class of distribution and not from arbitrary distribution). The subsection includes the scheme description and efficiency analyses in comparison with PoW\cite{PoW} 

\subsubsection{Setup}
\label{subsub:setup2}


Solution offered by Di Pietro is two party protocol and involves $C$(Client) as a prover and $S$(Server) as a verifier. He names a protocol as s-POW. Once $S$ receives the file for the first time it computes the $n$ number of challenges and stores the file on the disk. To compute the challenges $S$ keeps the hash-map data structure $\Im$. It maps files to the tuples and as a key it uses the hash of the file. Tuple contains  4 elements: $ptr$ -- the pointer on the file; $res []$ -- an array of generated challenges--called "responses" ($K$ bit string); $id_c$ -- the highest challenge computed so far ; $id_u$ -- number of challenges used so far.

\begin{figure}[ht] 
\begin{center}
\includegraphics[width=0.8\textwidth]{Di_Pietro_Hash_Map}
\caption{$\Im$ Hash-Map Structure}
\label{fig:Di_Pietro_Hash_Map} .
\end{center}
\end{figure}

The $S$ uses file digest $d$ (hash of the file), $id_c$ index and server's master key as an input for the  pseudo-random generator $F$ to produce random seed $s$. $s$ is an integer and $0 < s < file\_size $. The random seed $s$ serves for calculation of random position in file represent as bit vector and is unique of each challenge. At the end  using the random  position and the file as an input, $get-bit$ macros outputs the bit value. Concatenation of such outputs represents the response, which is $K$ bits log and is stored in $res[]$ array. Server computes $n$ number of responses at a time. This approach reduces I/O operations. Computation of responses takes place only, when the client uploads the file, which did not exist on server before or when  all the pre-computed responses are depleted.($id_c$ and $id_u$ control which responses is still valid and how many valid ones are remained.)\\\\
If client $C$ attempts to upload the file already located on server $S$, $S$ challenges the client, sends the random seed $s$ and waits for valid response. Client receives $s$ and use same $get-bit$ macro to produce a $K$ bit length response. If received response is the same as pre-computed one, the client succeeds to convince the server.\\\\
The s-POW protocol has two other modifications, which are designed to improve efficiency and are more convenient in particular cases. In one case the hash function which is used to calculate the key of the hash-map is substitute with calculation of response($K$ bit string) based on the file and the public seed $S_{pub}$. (This response serves as the key of hash-map) . In second case the file size is used as the key of the hash-map. The second solution is worthy only for large files, as the collision will be extremely high otherwise.

\subsubsection{Security And Efficiency}
\label{subsub:secAndeff2}

To demonstrate the security of s-POW protocol, Di Pietro shows the probability of adversary to convince the verifier and it is assumed that adversary already owns some large part of file. The Probability of adversary to guess the single bit for the $K$ bit length response is: $P(succ_1) = 1-\varepsilon(1-g)$, where $\varepsilon$ is fraction of file that is unknown for attacker and $g$ is the probability to guess unknown bit correctly. To convince the server, the adversary should guess the whole $K$ bit length response, and as guessing each bit from $K$ bit vector are independent events, the probability of convincing server is: $P(succ) = (1-\varepsilon(1-g))^K$. It means that the success probability  of adversary to convince the server depends on $K$, which is possible to tune based the security requirement. E.g. if the requirement is , $P(succ)\leq2^{-k}$, where $k$ is a security parameter, then $K =\lceil \frac{k ln 2}{\varepsilon(1-g)} \rceil $\\\\
Efficiency analyses comprises  the CPU computation, I/O  in bought client and server side and bandwidth. Di Pietro evaluates his proposed schemes(s-POW and s-POW1 \footnote{Modification of s-POW protocol, which uses $K$ bit string  instead of file hash as a key in hash-map}) and compares it with PoW.\footnote{"We have run our implementation of both schemes on a 64-bit RedHat box with an Intel Xeon 2.27GHz CPU, 18 GiB of RAM and an IBM 42D0747 7200 RPM SATA hard disk drive."}\cite{PoW} On client-side both s-POW and s-POW1 schemes are faster then PoW. The complexity up growth of s-POW and PoW schemes are equivalent of file size growth. It is reasonable because in both cases the dominant operation is hashing. While in s-POW1 computation cost becomes  constant for large files. As in the s-POW1 no hashing is used and only random disk access is needed to get required bit. For  visualization it is better to refer  the \hyperref[fig:POWvsPoW]{diagrams} provided by authors:
\begin{figure}[ht] 
\begin{center}
\includegraphics[width=1.0\textwidth]{POWvsPoW}
\caption{Comparison of running time on client-side}
\label{fig:POWvsPoW} .
\end{center}
\end{figure}

To demonstrate the server-side performance, it is convenient to divide it in two phases: initialization and regular execution. When the file is first uploaded on server-side, that represent initialization phase, all other communication between client and server is the regular execution phase. In initialization phase PoW and s-POW both performs file hashing. It follows with reduction and mixing phase and Merkle-tree calculation in case of  \hyperref[sub:Soltuion1]{PoW} and $n$ challenge computation in s-POW case. In s-POW when all pre-calculated challenges are used, server should pre-calculate them again but this is considered to be a part of regular execution phase. In regular execution phase the  \hyperref[sub:Soltuion1]{PoW}  performs Merkle-tree verification while the s-POW performs only look-up to get the correct tuple and the valid response from $res[]$  array. But in addition to look-up, s-POW needs to recalculate the challenges(responses). Dispite the pre-calculated challenges, which are done in order to decrees the file access, on server side PoW is faster then provided s-POW. It is also important to mention the storage efficiency, PoW need only Merkle-tree root value to store on server side, while s-POW for operation requires storing of the hash-map data structure.  Based on the performed analyses and execution, authors give the \hyperref[table:asymptoticAnalysis]{asymptotic analyses} of schemes. 


\begin{savenotes}
\begin{table}[!htpb]
\centering
\addtolength{\tabcolsep}{3pt}
\begin{tabular}{|L{6cm}|L{2.5cm}|L{2.5cm}|L{2.5cm}|}
\rowcolor{lightblue}
\hline
&PoW&s-POW&s-POW1\\
\hline
Client-side computation&$O(m)hash$ & $O(m)hash$ & $O(k)PRNG\footnote{Pseudorandom number generator}$ \\
\hline
Client-side I/O&$O(m)$&$O(m)$&$O(k)$\\
\hline
Server-side computation (initialization phase)&$O(m) hash$&$O(m) hash$&$O(nk)PRNG$\\
\hline
Server-side computation (regular execution phase)&$O(1)$&$O(nk) PRNG$& $O(nk)PRNG$\\
\hline
Server-side I/O (initialization phase)&$O(m)$&$O(m)$&$O(nk)$\\
\hline
Server-side I/O (regular execution phase)&$0$&$O(nk)$&$O(nk)$\\
\hline
Server-side storage&$O(1)$&$O(nk)$&$O(nk)$\\
\hline
Bandwidth&$O(k\ log\ k)$&$O(k)$&$O(k)$\\
\hline

\end{tabular}
\caption{Asymptotic analyses of schemes.POW,s-POW and s-POW1. $n$ is the number of challenges; $m$ is the file size; $k$ is a security parameter.}
\label{table:asymptoticAnalysis}
\end{table}
\end{savenotes}



\subsection{Solution \# 3}
\label{sub:Soltuion3}

The subsection reviews the solution offered by Chao Yang in his work "Provable Ownership of File in De-duplication Cloud Storage".\cite{POF} Provable Ownership of File, referred as a POF scheme, is two party protocol and helps the client to prove to the server that it indeed owns the file. The subsection first descripes the scheme, followed by the security and efficiency analyses. To demonstrate the  advantage of their scheme,  authors make comparison with \hyperref[sub:Soltuion1]{PoW} protocol.

\subsubsection{Setup}
\label{subsub:setup3}

POF is a cryptographic protocol that obliges a client to prove to the server that it owns the whole file. Client first sends the hash of the file to the server and if such hash already exist on server side, the POF protocol invokes and server challenges the client to prove the file possession. The POF consists of two phases: setup and challenge. In setup phase server decomposes the file $F$ in $f$ blocks. It chooses a random number  $R_c$  and generates session key $K_s = h_{sk}(R_c)$ (where $sk$ is pre-shared symmetric key) and two random seeds $S_1$ and $S_2$. Those randoms seeds are used in challenge phase to choose the blocks of the  file. Server provides the client with random number $R_c$, in order to generate the same session key $K_s$. Client generates the $K_s$ session key and  send back to server the $h_{K_s}(R_c, TS)||TS$ (where $TS$ is the current timestamp) value to confirm the generation of session key.  The session key $K_s$ is keep in secret and is used in challenge phases, while the $R_c$ could be deleted.\\\\
In challenge phase server sends the $c$ number of blocks($1 \leqslant c \leqslant f $) and two random seeds $S_1,S_2$ to the client. Random seeds $S_1$ and $S_2$ are used to produce block indices $i_\tau$ and dynamic coefficient $\delta_\tau$  where ($1 \leqslant \tau \leqslant c$). Client decomposes the whole  file in $f$ block $F=(b_1,b_2,b_3,..b_f)$ and computes the proof as hash of concatenation of the hashes of choose block and dynamic coefficients.  $V' = h_{K_ss}(h_{K_s}(b_{i_1},\delta_1)||h_{K_s}(b_{i_2},\delta_2)|| ...||h_{K_s}(b_{i_\tau},\delta_\tau))$. Client sends to the server the generated proof $V'$, server makes the same calculation, but uses the original file and generates the $V$ proof. If $V'=V$ the server is convinced that the client  owns the file. 
 


\subsubsection{Security And Efficiency}
\label{subsub:secAndeff3}


The POF has three major security requirement. First -- randomness of indices for the blocks of the file. Second -- the original file  must be involved for calculation the proof. And third -- the calculated proof should be different for different times. When all these three security requirement holds the scheme resistants to cheating is as high as the resistants of collision attack of the hash function used in POF. The authors provide the proof of the following theorem: "For the proposed POF scheme, the complexity for cheating of the ownership verification is at least as difficult as performing strong collision attack of the hash function"\\\\
To demonstrate the efficiency of the POF scheme, it is compared with \hyperref[sub:Soltuion1]{PoW}. The authors execute both schemes in the same setup\footnote{The experiments were conducted on an Intel 3.0GHz Intel Core 2 Duo system with 64KB cache, 1333MHz EPCI bus, and 2048MB of RAM. The system runs Ubuntu10.04, kernel version 2.6.34. We used C++ for the implementation. We also used the SHA256 from Crypto++ version 0.9.8b\cite{crypto}. The files are stored on an ext4 file system on a Seagate Barracuda 7200.7 (ST23250310AS) 250GB Ultra
ATA/100 drive.} and demonstrate the results for each of them in $milliseconds$. Two protocols computation time is decomposed in three parts: client, server and network computation time.\\\\ 
Client computation time is covered in details. For POF protocol it includes, time to read randomly chosen parts of original file, key derivation time and time to compute the proof. While PoW for simplicity, includes just whole file reading and Merkle-tree calculation time on original file.(reducing and mixing phases are omitted) And even in this scenario POF is more time efficient on client side. In POF file's portions reading time increases as file size increase, but the time for proof computation could stay the same and does not depend on file size. For PoW -- the whole file reading time and Merkle-tree computation time, both increase as file size grows. Based on results,(see \hyperref[Appendix 1] {Appendix 1})  we can demonstrate  the difference in client time computation (in average), between POF and PoW schemes. The schemes had run on different size of files and the file size was doubling each time. The starting size was $0.015625MB$ and it had increased till $1024$MB. We can see that the file size was increased 65536 times. As file size was growing the time consumption was changing. In case of POF  for the disk reading , the time was increased approximately $37$ times and key derivation and  proof computation time was remained mostly constant(around $0.62 ms$). So the total time was increased $8$ times (at stating point the total time was $0.77$ and at the ending point -- $6.15 ms$). In case of PoW -- disk reading was increased $28684$ times and Merkle-tree computation approximately $40955$ times, overall $39296$ time for total time. (at stating point the total time was $0.66$ and at the ending point -- $25926.39 ms$). \\\\
Server side computation time is considered to be same or less then client side computation for POF. In case of PoW it is to check the Merkle-tree authentication signature, which is not time consuming. Network transmitting time, is the time required to transfer the data generated by protocols. In the given setup the data generated for both protocols was less then $1KB$. The server and network times both are negligible and based on the client computation time it was demonstrated the POF is more efficient then PoW.

\subsection{Solution \# 4}
\label{sub:Soltuion4}

The subsection covers solution offered by Jia Xu and his colleagues in the paper work "Weak Leakage-Resilient Client-side Deduplication of Encrypted Data in Cloud Storage".\cite{Leakage-Resilient} In previous solutions reviewed in this work, the server is considered as a honest player and effort is directed to prevent malicious clients. While this paper address both client and server side threats. Paper underlines the importance of confidentiality of user's sensitive data. It claims that, the remote storage provider should not have access to the users sensitive information and proposes the solution of Proof of Work protocol over the encrypted files on client side. Jia Xu expends the security restriction of Halevi's \hyperref[sub:Soltuion1]{PoW}  protocol, from specific class of file distribution to arbitrary file distribution. But on the other hand it restricts the data leakage size and security holds if the leakage takes place only before the protocol starts, while in PoW leakage could happen any time, but not the during protocol communication.\\\\
Using encryption on client side delivers confidentiality of sensitive information but, on the other hand it rises the risk of \textit{Poison Attack}, also known as \textit{Target Collision attack}.\cite{PoisonAttac} When encrypted file is uploaded on server side, server is not able to check consistency between file and meta-data (e.g. hash of the file). This feature opens possibility to attacker to substituted the encrypted file with the same size malicious one. And if later the owner of the file retrieves it, she gets poisoned file not the original one. Solution demonstrated in this subsection, takes into consideration these type of threats and  offers solid security over some restricted leakage conditions.


\subsubsection{Setup}
\label{subsub:setup4}

The Weak Leakage-Resilient Client-side Deduplication scheme, is refereed as $CSD$ shortly and is represented with four probabilistic polynomial-time algorithms $E, D, P, V$ .  $E$ is an encryption algorithm: $E(F,1^\lambda) 	\rightarrow (\tau, F_0, F_1)$, where $F$ is file, $\lambda$ is security parameter, $\tau$ encryption key and $C_0 and C_1$ are cipher-texts. $C_0$ -- encrypted key and $C_1$ -- encrypted file. $D(\tau,C_1) \rightarrow F $ -- decodes $C_1$ cipher. $P(F) \rightarrow y_0,  y_0 \in \{\tau,\perp\} $ -- prover algorithm , which interacts with verifier algorithm and outputs the encryption key $\tau$. $V(C_0) \rightarrow (y_1,y_2), y_1 \in \{Accept; Rejectg\} and y_2 \in \{hash(C_1),\perp \}$ -- verifier algorithm, which interacts with prover algorithm.\\\\
The $CSD$ protocol involves the client and the server. When a client uploads the file first time on the server, it generates the random AES\cite{AES} key $\tau$ and two ciphers: $C_F$ and $C_\tau$. Where $C_F$ is an encrypted file, using AES encryption and generated  key $\tau$  and is almost as large as original file. While  $C_\tau$ is the encrypted random key $\tau$ generated by client, using some custom encryption method and the file $F$ as a key. And the size of $C_\tau$ is small number -- upper bounded by $|\tau|$. After the key and ciphers are generated, client sends hash value of original file $F$, together with two cipher $C_F$ and $C_\tau$.  Server receives the data form client, stores the encrypted file $C_F$ in secondary storage(large but slow storage). While meta-data of encrypted file is stored in primary storage and is represented as key-value pair entry in the lookup database: $(key = hash(F); value = (hash(C_F), C_\tau))$ , where $hash(C_F)$ is calculated by server and $hash(F)$ and $C_\tau$ is received form client.\\\\
If client tries to upload the file -- already located on server side, it first sends the $hash(F)$ to the server.  Based on the received $hash(F)$, the server identities  the tuple $(hash(C_F),C_\tau)$ in the lookup database. It retrieves the $C_\tau$ value and sends back to client. If the client indeed owns the file, she can decrypt the cipher $C_\tau$ using the file as an decryption key and retrieve the ASE key $\tau$. After obtaining the AES key $\tau$, client encrypts the original file, computes the hash of it and sends the $hash(C_F)$ as a proof to the server. Server compares the received $hash(C_F)$ hash value, with the one kept in its lookup database. If the values are equal then the client gains the access to the file, otherwise access is forbidden. After client is identified as honest, she can remove the original file and keep only the ASE key $\tau$, which she will use later for decrypting the downloaded file.\\\\



\subsubsection{Security And Efficiency}
\label{subsub:secAndeff4}

To demonstrate the security of the $CSD$ scheme, authors use the game-based security proof. The security definition states that, adversary can not learn anything new about the a single bit of the file( thereof about the file in  whole), from client-side deduplication process, besides the side channel leakage. Security game $G^{CSD}_A$ between challenger and PPT $A$ adversary consists of two learning and two guessing phase. In first learning phase, adversary receives the output of some PPT function $y$ form the challenger. $y < (\varepsilon_0 - \varepsilon_1)$ , where  $\varepsilon_0$ is a minimum min-entorpy of the file and  $(\varepsilon_0 - \varepsilon_1)$ -- is a max length of bits adversary is allowed to learn about the file. For it's part $\varepsilon_0>\varepsilon_1  \geq\lambda$, where $\lambda$ is a security parameter. Adversary chooses the $v$ indices ($i_1, ...i_v$)  and the challenger chooses the subsequence of the file $\alpha \in \{0,1\}^v$, such the the each bit chosen based on $v$ indices from that subsequence $\alpha$ corresponds the bit from the file $F$ also chosen based those indices. The challenger chooses the bit  $b \in \{0,1\}$ and sets $\alpha_b=\alpha$ and $\alpha_{1-b}  \in_R \{0,1\}^v$ and send those $\alpha_0$ and $\alpha_1$ to the adversary. It is followed with the first guessing phase, where an other extractor $A_*$ produces the guess of $b$ bit, $b_{A_*} \in \{0,1\}$. In the second guess phase the adversary tries to guess the $b$ bit, $b_A \in \{0,1\}$.\\\\
The $CSD$ is secure in $(\varepsilon_0, \varepsilon_1)$ if the probability of guessing $b$ single bit by extractor $A_*$ plus some negligible in security parameter $negl(\lambda)$, is grater or equal then probability of adversary $A$ guessing the same $b$ bit: $Pr[b_A=b]\leq Pr[b_{A_*}=b]+negl(\lambda)$. Authors construct the secure $CSM$ in the paper and have provide the proof of the security statement in there work.\\\\
The $CSD$ scheme efficiency is measured based on running them of prover $P$ and verifier $V$ interactive algorithm.\footnote{"The test machine is a laptop computer, which is equipped
with a 2.5GHz Intel Core 2 Duo mobile CPU (model T9300), a 3GB PC2700-800MHZ RAM and a 7200RPM hard disk. The test machine runs 32 bits version of Gentoo Linux OS with kernel 3.1.10. The file system is EXT4 with 4KB page siz" \cite{Leakage-Resilient}}It is compared with the running time of transferring files without deduplication or encryption. The \hyperref[fig:CSD]{graph} provided by authors shows, that it is more efficient to use $CSD$ scheme for secure deduplication rather then avoid it at all and transfer whole files via network.\\\\
\begin{figure}[ht] 
\begin{center}
\includegraphics[height=310pt,width=400pt]{CSD}
\caption{$CSD$ efficiency graph}
\label{fig:CSD} .
\end{center}
\end{figure}

\pagebreak

\subsection{Solution \# 5}
\label{sub:Soltuion5}
The subsection reviews the solution offered by Nesrine Kaaniche and Maryline Laurent in their paper work "A Secure Client Side Deduplication Scheme in Cloud Storage Environments".\cite{Kaaniche} Like the last solution, it addresses both the malicious clients and the curious-servers issues. It provides the means to control the integrity of both players, neither server no client gets fooled. In addition it address the private file shearing issues. The subsection first covers how the solution operates and provides security and efficiency analyses later.

\subsubsection{Setup}
\label{subsub:setup5}


The security scheme is implemented on OpenStack Swift \footnote{The open-source cloud storage project - \url{https://www.swiftstack.com/product/openstack-swift}} platform and it is based on  convergent encryption concept\cite{CovergentEnc}, but for identification it uses Merkle-tree signature. The authors give a clear and simple \hyperref[fig:ConvMT]{scheme} of cloud storage architecture.
\begin{figure}[ht] 
\begin{center}
\includegraphics[height=210pt,width=400pt]{ConvMT}
\caption{Client uploads and stores the data on server and Users are allowed to access (depend on their access rights) the content of stored data}
\label{fig:ConvMT} .
\end{center}
\end{figure}
When data owner desires to outsource the file $F$ on the remote server, he first encrypts it, and uses the content hash $K_F$ as an encryption key. He builds the Merkle-tree on the encrypted file and  uses the root value $MT_F$ as the file identifier, which is unique on the remote storage. Client sends the $MT_F$ identifier\footnote{The client also adds nonce $n$ to the identifier to avoid a replay attack} to the server and checks weather the file already is located on the server or not. The server checks in the database and if the received identifier does not matches, asks client to upload the file. The client in his turn, sends the encrypted file and the encryption key $K_F$ -- encrypted by the public keys of the authorized users (users with whom client desires to share the file). Later the encrypted key $K_F$ is included in the file meta-data. After the server stores the file, it sends the acknowledgement message to client, which includes the stored file's URI(Uniform Resource Identifier).\footnote{The solution is implement on Swift, which is an object store, so each file could be reached by URI}\\\\
If the file is already located on the server, then the server sends to the client some number of random indecies of Merkle-tree leaves. The client calculates the sibling paths of chosen leaves  and sends (leaves together with sibling paths) as proof of ownership to the server. Server validates the received proof and if it holds, it sends acknowledgment on storing the file and the URI of requested file to the client. Otherwise, client is failed to access the file.\\\\
When client desires to access the outsourced data he owns -- he sends the file URI to the server. The server, check in its database , weather the client owns the file. If the client is the owner of the requested file, server sends encrypted file to the client. After receiving the file, client first extracts the file meta-data(i.e. encrypted $K_F$ encryption key), decodes the encrypted $K_F$ key with own private key and use $K_F$ to decode extracted file.\\\\




\subsubsection{Security And Efficiency}
\label{subsub:secAndeff5}
Authors offered very brief overview of security, without any game-based or simulation-based security proof. They address three main issues in their scheme: Data confidentiality, privacy and access control.  When client wants to store new data to the remote storage, he calculates the Merkle-tree  over the encrypted file and sends the root value together with nonce $n$. Nonce  prevents from replay attacks\footnote{"Replay attacks are the network attacks in which an attacker spies the conversation between the sender and receiver and takes the authenticated information"}(stilling the identifier), while transferring the data. But if the adversary still will be able to obtain the identifier (root value of the Merkle-tree), he is required to proof the ownership of the file and this proof is based on Merkle-tree lemma \cite{MerkleLemma} and prevents malicious client to access the confidential data. Scheme also prevents curios  servers to rich  the users' private data and built users' profile. The server is not able to access the data, because it is encrypted and the encryption key is secured with asymmetric encryption. Access control is managed by embedding the symmetric encryption key $K_F$ encrypted by public keys of the users who are authorized to access the file.\\\\
To evaluate the performance of the solution, authors implement the OpenStack swift framework and integrate it with their own scheme for security.\footnote{"For our tests, we used 1000 samples in order to get our average durations. In addition, we conducted our experiments on an Intel core 2 duo, started on single mode,where each core relies on 800 MHz clock frequency (CPU).\cite{CovergentEnc}"} The scheme's performance time is decomposed in two parts: Client and Server computation time. The client computation time is consist of the data encryption/decryption time and data upload/download time. For encryption/decryption of file the scheme uses symmetric AES(Advanced Encryption Standard) in CBC(Cipher Block Chaining) mode.
The authors examine the performance of file encryption/decryption using different key sizes(key size = $128, 192 or 256 bits$) of AES encryption and different file input sizes (file size = $k * 10^5$ where $k \in \{ 1,2,..10\}$ ).  The results  exposes -- the computation time is dependent on a key size and a file size and as they are increasing the time of computation increases as well. The encryption/decryption time for the smallest key($128 bits$) and the smallest input file size($0.1 mb$) is 1 ms, while with the largest key size and input file size (key size = $256 bit$, file size = $1mb$) is less then 12 ms.  The authors examine the upload/download time and the results shows -- uploading time is greater then downloading time. Also time remains constant for small files (file size < $5*10^4$ bits), while for  larger files it depends on file size and time increase as the file size increases.
To compare the cryptographic operations to the file transfer operation on client-side, it is easy to see that cryptographic operations are consuming much less time file upload/download. And the comparisons that is given in the paper \cite{CovergentEnc} claims, that encrypting of the  $0.8 mb$  size file takes $0.1ms$, while uploading it takes $10s$, which means the encryption operation is 1\% of uploading operation. For securing the AES encryption key, the scheme uses ECC (Elliptic Curve Cryptography).\cite{ECC}  Unfortunate the paper\cite{CovergentEnc} doe snot covers the server time consumption and there is no any comparisons with other schemes. 


\subsection{Solution \# 6}
\label{sub:Soltuion6}

This subsection is dedicated to a paper work "A Tunable Proof of Ownership Scheme for Deduplication Using Bloom Filters".\cite{BF} The paper offers  the data ownership proof scheme based on Bloom Filter.\cite{BF-Original} The bloom filter is a time and space efficient data structure. It serves to identify, weather the element is  the member of the set or not. The issue with this data structure -- it has false positives. The element which does not belongs to the set, could be considered as the member, but never vice-versa. Bloom filter allocates $s$ bits length vector (all bits set to $0$) and uses $n$ number of different evenly distributed hash factions. To add the element in bloom filter, the element is hashed with the $n$ different hash functions. The results of hashes is the indecies in the $s$ bits length vector and in those indecies bits are set to $1$. To check if the particular element belongs the set, it must be hashed using the $n$ different hash functions to produce the indecies. After the indecies are produced, the vector is examined in those indecies and if all the bits are set to $1$, then the  element belongs the set.


\subsubsection{Setup}
\label{subsub:setup6}
The Proof of Ownership Scheme for Deduplication Using Bloom Filters, also referred as bf-POW is the two party protocol. It represents an interaction between $C$ as a client and the $S$ as a server. There are two scenarios, first when uploading the file first time on the server and second trying to upload the file already located on the server. In the first scenario, client  sends the hash of the file  $h_f$  to the server. The server identifies that the file is unique, based on the fingerprint $h_f$ and asks the client to upload the whole file $f$ and initializes all the required data structures, including bloom filter. After receiving the file $f$, server divides it into the equal size chunks, then computes the tokens, using some hash function: $H:\{0,1\}^B\to\{0,1\}^l$, where the $B$ is a chunk size and $l$ is a token size. And finally the indecies are generated form token using some pseudo random function, $PRF: \{0,1\}^l \to \{0,1\}^{n}$ where $n$ is a positive  integer. Those indecies are inserted in the bloom filter $BF$ data structure. The server keeps the associative array $A$, which use the hash of the file $h_k$ as a key and the tuple $\{f,BF,AL\}$ as a value. In the tuple the $f$ is the content of the file, $BF$ represents the bits in the bloom filter and the $AL$ is the list of the client identifiers $id(C)$, who are owning the file. After the file is uploaded first time on the server , the entry is added to the array $A$.\\\\
In the second scenario, when the file is already located on the server, there is no need to upload the file, instead the client must proof that he indeed owns it. The server initializes the array $pos$, which holds the randomly chosen chunks' indecies and $J$ is the length of the $pos$ array. The server sends $pos$ array to the client and waits for the tokens generated based on $pos$ array. The client performs the same operation as the server did in order to generate the tokens and creates the array $res$ which holds the generated tokens.  $res[i]\gets H(f[pos[i]])$ where $ 0 \leq i < J$ and $J$ is the number of randomly chosen indecies. The client sends $res$ array to server and the server generates the indeces using the $PRF$ and the tokens form $res$ array and checks whether each output belongs to bloom filter or not. If all outputs belongs to bloom filter then the client is considered as the file owner and the $id(C)$ is added to the $A[h_f].AL$ list. Otherwise the client fails to proof the file ownership. Later when client request to download the file , the server will check, if the $id(C) \in A[h_f].AL$



\subsubsection{Security And Efficiency}
\label{subsub:secAndeff6}


To demonstrate the security of the bf-POW scheme, authors evaluate a probability of the adversary $\bar{C}$ convincing the server without owning the file. The adversary is allowed to communicate with file owners and receive some information about the file (but not during th protocol takes place). The probability that adversary knows the $B$ bits long randomly chosen chunk of the file is $p$. And the probability to guess the randomly chosen byte is $g$. In order to pass the proof the adversary $\bar{C}$ should provide $J$ tokens, based on randomly chosen $J$ chunks by the server. And 
those tokens used as seeds of $PRF$ should produce the indecies which will belong to the bloom filter for the given file $f$. In order to succeed there are two options. First $\bar{C}$ should produce the correct token and the second -- occurrence of false positive when checking for bloom filter membership. The probability of false positive in bloom filter is -- $p_f$. The $p_f$ depends on the size of the bloom filter and the number of hash functions.\footnote{If $J$ is a total number of elements that could be inserted in the bloom filter and $p_f$ is the probability of false positives, then the size of bloom filter is $s=\lceil - \frac {N\ ln\ p_f} {(ln2)^2}\rceil$ and the number of independet hash functions is $n=\lceil\frac{s} {N}ln2\rceil$} The probability to produce one randomly chosen token from the $J$ tokens correctly is $P(in_i)=P(tok_i)+p_f*P(\bar{tok_i}) \  (1)$ where $in_i$ and $tok_i$ are both events: $in_i$ --  i-th generated token, is a seed of the index which is the member of bloom filter; $tok_i$ -- adversary generates token correctly. In its part probability of event $tok_i$ is decomposed as the probability of knowing the chunk and probability of guessing it. The $p$ is the probability that the adversary knows the $i$-th chunk.  If adversary knows the $i$-th chunk, it means that he is able to compute the $i$-th token correctly. If he does not know, he should guess. Probability of guessing $B$ bits long chunk is lower, then probability of guessing $l$ bit token directly, $g^B << 0.5^l$. From adversary perspective better to guess directly a chunk. And $P(tok_i)=p+(1-p)*0.5^l \  (2) $ In order to succeed adversary needs to produce $J$ number of tokens form randomly chosen chunks. To compute the probability of success, we use these two formulas : $(1)$ and $(2)$\\\\
$P(succ)=P(in_i)^J=(P(tok_i)+p_fP(tok_i))^J=(p+(1-p)*0.5^l + p_f*(p+(1-p)*0.5^l))^J=(p+(1-p)*0.5^l)+p_f(1-p)(1-0.5^l)^J=(p+(1-p)(0.5^l)+p_f(1-0.5^l))^J$\\\\
And it is possible to choose $J$ in such a way that the $P(succ) \leq 2^{-k}$ , which is negligible in the security parameter $k$.\\\\
Authors provide the asymmetric analyses and the results of the experiments of bf-POW, in comparison with \hyperref[sub:Soltuion1]{POW} and \hyperref[sub:Soltuion2]{s-POW} schemes. Each time the client tries to upload the file, he hashes it. Also the client is required to calculate the tokens, meaning the $J$ times hashing operation over the $l$ length chunks. On server side there are two phases, with different computation complexity in each phase. In initialization phase (when the file is uploaded first time on server side) the dominant cost is the hashing operations used to insert elements in bloom filter. As there are $n$ different functions, it requires $n$ hashing operations. Also in initialization phase server should load the whole file into the memory, while in execution phases (when the file is already located on the server) the server needs only bloom filter to be loaded in the memory, and the indecies are generated by hashing the tokens received from client.(res[] array). In terms of bandwidth efficiency, bf-POW generates $J$ tokens. (the client sends tokens to the server in order to proof ownership). 
The complexity of the schemes is represented in big-O notation, is provided by authors in the following  \hyperref[table:asymptoticAnalysisBF-POW]{table} : 

\begin{savenotes}
\begin{table}[!htpb]
\centering
\addtolength{\tabcolsep}{3pt}
\begin{tabular}{|L{5cm}|L{2.5cm}|L{2.5cm}|L{3.5cm}|}
\rowcolor{lightblue}
\hline
&PoW&s-POW&bf-POW\\
\hline
Client computation&$O(F)hash$ & $O(F)hash$ & $O(F)hash$ \\
\hline
Client I/O&$O(F)$&$O(F)$&$O(F)$\\
\hline
Server init computation &$O(F) hash$&$O(F) hash$&$O(F)hash$\\
\hline
Server regular computation &$O(1)$&$O(nk) PRF$& $O(\frac{l*k*(log1/p_f)}{p_f})hash$\\
\hline
Server init I/O &$O(F)$&$O(F)$&$O(F)$\\
\hline
Server regular I/O &$0$&$O(nk)$&$O(0)$\\
\hline
Server memory usage&$O(1)$&$O(nk)$&$O(\frac{log(1/p_f)}{l})$\\
\hline
Bandwidth&$O(k\ log\ k)$&$O(k)$&$O(\frac{lk}{p_f})$\\
\hline

\end{tabular}
\caption{Asymptotic analyses of schemes:POW,s-POW and bf-POW. $F$ is the file size; $k$ is a security parameter; $n$ is number of challenges in s-POW; $m$ is the file size; $l$ is a $PRF$ output size; $p_f$ is a probability of false positive in BF}
\label{table:asymptoticAnalysisBF-POW}
\end{table}
\end{savenotes}
 
The authors have implement \hyperref[sub:Soltuion1]{POW}, \hyperref[sub:Soltuion2]{s-POW} and their bf-POW schemes, in order to compare the time performance results to each other. \footnote{"The benchmarks are run on an Intel Xeon 2.27GHz CPU with 18 GiB of RAM running RHEL Server release Santiago (6.1). The input fles contain random data, and their size ranges from 1 MiB to 4 GiB, with the size doubled at each step."} The results show that in bf-POW the better performance is dependent on the size of token, which in its part also decries bandwidth consumption. Smaller tokens influence the size of bloom filter and increase it in order to keep the security in place.(decries the probability of false positives.)  In the worst case bloom filters require  $2MB$ additional storage per file. Bf-POW is always faster then s-POW scheme on server side and is the fastest  among those three on server side for specific $p$ ($p$ is the probability that adversary knows the randomly chosen chunk of the file). On client side, bf-POW is faster then POW and is a litter slower then s-POW.
 
 


\subsection{Solution \# 7}
\label{sub:Soltuion7}

\subsubsection{Setup}
\label{subsub:setup7}

\subsubsection{Security And Efficiency}
\label{subsub:secAndeff7}


%Outlines the main thing your thesis does. Your thesis describes a novel algorith for X? Your main contribution is a case study that replicates Y? Describe it here.
\begin{itemize}
 \item Proofs of Ownership in Remote Storage Systems - 2011 + 
 \item Boosting Efficiency and Security in Proof of Ownership for Deduplication - 2012 +
 \item Provable Ownership of File in De-duplication Cloud Storage - 2013 +
 \item Leakage-Resilient Client-side Deduplication of Encrypted Data in Cloud Storage - May 8,2013 +
 \item A Secure Client Side Deduplication Scheme in Cloud Storage Environments - 2014 +
 \item A Tunable Proof of Ownership Scheme for Deduplication Using Bloom Filters - 2014 +
 \item An efficient confidentiality-preserving Proof of Ownership for Deduplication -2014
 
\end{itemize}

\pagebreak

%------------------------------EVALUATION-----------------------------------
\section{Evaluation}
\label{sec:5}
Demonstrate why the developed framework  is secure and efficient. 


\begin{enumerate}
 \item Time complexity evaluation.
 \item Cost analyses. 
\end{enumerate}



%Describes why your approach really solves the problem it claims to solve. You implemented a novel algorithm for X? 
%This chapter describes how you ran it on a dataset and reports the results you measured. You replicated a study? This chapter gives the results and your interpretations.

%The Appraoch and Evaluation chapters contain the meat of your thesis. Often, they make up half or more of the pages of the entire document.

\pagebreak

%------------------------------FUTURE WORK-----------------------------------
\section{Future Work}
\label{sec:6}

 In science folklore, the merit of a research question is compounded by the number of interesting follow-up research questions it raises. 
 So to show the merit of the problem you worked on, you list these questions here.
 %If you don’t care about research folklore (I did not as a student),
 %this chapter is still useful: whenever you stumble across something that you should do if you had unlimited time, but cannot do since you don’t, you describe it here. 
 %Typical candidates are evaluation on more study objects, investigation of potential threats to validity, … 
 %The point here is to inform the reader (and your supervisor) that you were aware of these limitatons. Limit this chapter to very few pages. 
 %Two is entirely fine, even for a Master’s thesis.
\pagebreak


%------------------------------CONCLUSIONS-----------------------------------
\section{Conclusions}
\label{sec:7}
Short summary of the contribution and its implications. The goal is to drive home the result of your thesis.
Do not repeat all the stuff you have written in other parts of the thesis in detail. Again, limit this chapter to very few pages. 
The shorter, the easier it is to keep consistent with the parts it summarizes.

\pagebreak

%------------------------------REFERENCES-----------------------------------

\addcontentsline{toc}{section}{References}


\bibliographystyle{IEEEtran}
\bibliography{bibi.bib}


\pagebreak

%-----------------------------APPENDIX--------------------------------

\appendix

\section{Appendix 1}

\label{Appendix 1}

\subsection*{POF -- PERFORMANCE MEASUREMENTS AND COMPARISON}



\begin{savenotes}
\begin{table}[!htpb]
\centering
\addtolength{\tabcolsep}{3pt}
\begin{tabular}{|L{2cm}|L{2cm}|L{2cm}|L{2cm}|L{2cm}|L{2cm}|L{2cm}|}
\rowcolor{lightblue}
\hline

&\multicolumn{3}{|C{6.5cm}}{ PoW (ms)}&\multicolumn{3}{|C{6.5cm}|}{POF (ms)}\\ \hline


Size (MB)&Disk Read&Merkle Tree&Total&Disk Read&Algorithm&Total\\ \hline

0.015625&0.09&0.57&0.66&0.15&0.62&0.77 \\ \hline
0.03125&0.13&1.07&1.2&0.16&0.42&0.58 \\ \hline
0.0625&0.19&1.73&1.92&0.17&0.62&0.79 \\ \hline
0.125&0.34&4.01&4.35&0.2&0.63&0.83 \\ \hline
0.25&0.62&6.82&7.44&0.24&0.63&0.87 \\ \hline
0.5&1.17&12.51&13.68&0.27&0.58&0.85 \\ \hline
1&2.03&21.08&23.11&0.29&0.62&0.91 \\ \hline
2&4.44&42.46&46.9&0.31&0.62&0.93 \\ \hline
4&8.19&84.46&92.65&0.55&0.63&1.18 \\ \hline
8&14.76&168.43&183.19&0.66&0.65&1.31 \\ \hline
16&28.62&334.88&363.5&0.82&0.64&1.46 \\ \hline
32&56.75&669.38&726.13&1.32&0.67&1.99 \\ \hline
64&112.58&1352.01&1464.59&4.34&0.64&4.98 \\ \hline
128&223.08&2692.07&2915.15&5.56&0.65&6.21 \\ \hline
256&437.84&5393.64&5831.48&2.11&0.65&2.76 \\ \hline
512&1269.46&10932.49&12201.95&5.53&0.64&6.17 \\ \hline
1024&2581.56&23344.83&25926.39&5.52&0.63&6.15 \\ \hline


\end{tabular}
\caption{Client Computation Time -- POF vs PoW }
\label{table:asymptoticAnalysis}
\end{table}
\end{savenotes}


\end{document}
