
\documentclass[12pt]{article}     
\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage[top=2.5cm, bottom=2.5cm, left=3cm, right=3cm]{geometry}
\usepackage{titlesec}
\usepackage{longtable}
\usepackage[table,xcdraw]{xcolor}
\usepackage{todonotes}
\usepackage{float}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{tocloft}
\usepackage{amssymb}
\renewcommand{\labelitemi}{\tiny$\blacksquare$} %For square itemized lists
\usepackage{caption} 
\captionsetup{labelsep=period}
\usepackage{verbatimbox} %To put program code in the center using Verbatim
\titlelabel{\thetitle.\quad}
\usepackage{times}
\usepackage{fancyhdr}
\setlength{\parindent}{0cm}
\usepackage{setspace}
\onehalfspacing
\setlength{\parskip}{\baselineskip}
\usepackage{amsmath} 
\usepackage{amsthm}
% Packages for building tables and tabulars 
\usepackage{array}
\usepackage{tabu}   % Wide lines in tables
\usepackage{xspace} % Non-eatable spaces in macros
\usepackage[colorlinks=true,linkcolor=blue]{hyperref}
\usepackage[all]{hypcap}
\usepackage{url}
% Packages for defining colourful text together with some colours
%\usepackage[table,xcdraw]{xcolor}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\definecolor{lightblue}{rgb}{0.95,0.97,1.0}
\definecolor{darkblue}{rgb}{0.90,0.92,1.0}
\usepackage{color}

% Standard package for drawing algorithms
% Since the thesis in article format we must define \chapter for
% the package algorithm2e (otherwise obscure errors occur) 
\let\chapter\section
\usepackage{algorithm2e}

% Macros that make sure that the math mode is set
\newcommand{\typeF}[1] {\ensuremath{\mathsf{type_{#1}}}\xspace}
\newcommand{\opDiv}{\ensuremath{\backslash \mathsf{div}}\xspace} 
\usepackage{listings}

\lstset{ 
  %language=python,                % the language of the code
  language=C++,
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  %numbers=left,                   % where to put the line-numbers
  %numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
  numberstyle=\tiny\color{gray}, 
  stepnumber=1,                    % the step between two line-numbers. If it's 1, each line 
                                   % will be numbered
  numbersep=5pt,                   % how far the line-numbers are from the code
  backgroundcolor=\color{white},   % choose the background color. You must add \usepackage{color}
  showspaces=false,                % show spaces adding particular underscores
  showstringspaces=false,          % underline spaces within strings
  showtabs=false,                  % show tabs within strings adding particular underscores
  frame = lines,
  %frame=single,                   % adds a frame around the code
  rulecolor=\color{black},		   % if not set, the frame-color may be changed on line-breaks within 
                                   % not-black text (e.g. commens (green here))
  tabsize=2,                       % sets default tabsize to 2 spaces
  captionpos=b,                    % sets the caption-position to bottom
  breaklines=true,                 % sets automatic line breaking
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  %title=\lstname,                 % show the filename of files included with \lstinputlisting;
                                   % also try caption instead of title
                                   % also try caption instead of title
  keywordstyle=\color{blue},       % keyword style
  commentstyle=\color{dkgreen},    % comment style
  stringstyle=\color{mauve},       % string literal style
  escapeinside={\%*}{*)},          % if you want to add a comment within your code
  morekeywords={*,game, fun}       % if you want to add more keywords to the set
}

\usepackage{multirow}
\setlength{\tabcolsep}{0pt}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1-\arrayrulewidth\relax}}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1-\arrayrulewidth\relax}}

\usepackage{booktabs,fixltx2e}
\usepackage{tikz}
%used for ex. for m prime
\usepackage{flexisym}
\usepackage{footnote}

\usepackage{arydshln}
\usepackage{placeins}
\usepackage{enumitem}
\usepackage{cleveref}
\crefformat{footnote}{#2\footnotemark[#1]#3}
\usepackage{IEEEtrantools}

\begin{document}
\bstctlcite{IEEEexample:BSTcontrol}
%------------------------------TIITELLEHT---------------------------------
\thispagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\headheight = 57pt
\footskip = 11pt
\headsep = 0pt

\chead{
 \textsc{\begin{Large} %Tekst suurtähtedega ja suuremaks
	Tallinn University of Technology\\
	\end{Large} }
	Department of Computer Science\\	
	TUT Center for Digital Forensics and Cyber Security
}
\vspace*{7 cm}

\begin{center}
ITC70LT\\[0cm]

Gvantsa Grigolia 144965\\
\vspace{15pt}
\begin{LARGE}
\textsc{Evaluation of data ownership solutions in remote storage.\\}
\end{LARGE}
\vspace{10pt}
Master Thesis\\[2cm]
\end{center}

\begin{flushright}
Supervisor: Ahto Buldas\\[0cm]
Professor \\[0cm]
\end{flushright}

\cfoot{Tallinn 2016} 
\pagebreak

%---------------------------AUTORIDEKLARATSIOON-------------------------
\section*{\begin{center}
 Autorideklaratsioon
\end{center}}


Autorideklaratsioon on iga lõputöö kohustuslik osa, mis järgneb tiitellehele.
Autorideklaratsioon esitatakse järgmise tekstina:

Olen koostanud antud töö iseseisvalt. Kõik töö koostamisel kasutatud teiste autorite tööd, olulised seisukohad, kirjandusallikatest ja mujalt pärinevad andmed on viidatud. Käsolevat tööd ei ole varem esitatud kaitsmisele kusagil mujal.

Autor: [Ees$-$ ja perenimi]

[\today]
\pagebreak

%---------------------------ANNOTATION---------------------------------
\section*{\begin{center}
Annotatsioon
\end{center}}

Annotatsioon on lõputöö kohustuslik osa, mis annab lugejale ülevaate töö eesmärkidest, olulisematest käsitletud probleemidest ning tähtsamatest tulemustest ja järeldustest. Annotatsioon on töö lühitutvustus, mis ei selgita ega põhjenda midagi, küll aga kajastab piisavalt töö sisu. Inglisekeelset annotatsiooni nimetatakse Abstract, venekeelset aga


Sõltuvalt töö põhikeelest, esitatakse töös järgmised annotatsioonid:
\begin{itemize}
\item kui töö põhikeel on eesti keel, siis esitatakse annotatsioon eesti keeles mahuga $\frac{1}{2	}$ A4 lehekülge ja annotatsioon \textit{Abstract} inglise keeles mahuga vähemalt 1 A4 lehekülg;
\item kui töö põhikeel on inglise keel, siis esitatakse annotatsioon (Abstract)  inglise keeles mahuga $\frac{1}{2}$ A4 lehekülge ja annotatsioon eesti keeles mahuga vähemalt 1 A4 lehekülg;
\end{itemize}

Annotatsiooni viimane lõik on kohustuslik ja omab järgmist sõnastust:

Lõputöö on kirjutatud [mis keeles] keeles ning sisaldab teksti [lehekülgede arv] leheküljel, [peatükkide arv] peatükki, [jooniste arv] joonist, [tabelite arv] tabelit.
\pagebreak


%-----------------------------ABSTRACT-----------------------------------

\section*{\begin{center}
Abstract
\end{center}}
Võõrkeelse annotatsiooni koostamise ja vormistamise tingimused on esitatud eestikeelse annotatsiooni juures.

The thesis is in [language] and contains [pages] pages of text, [chapters] chapters, [figures] figures, [tables] tables.
\pagebreak

%---------------------Glossary of terms and Abbreviations---------------------

%\section*{\begin{center}
%Glossary of Terms and Abbreviations
%\end{center}}
%Lühendite  ning  mõistete  sõnastikku  lisatakse kõik töö põhitekstis kasutatud  uued  ning  ka mitmetähenduslikud üldtuntud terminid. Näiteks inglisekeelne lühend PC  võib tähendada nii Personal Computer kui ka Program Counter, sõltuvalt kontekstist. Lühendid ja mõisted esitatakse tabuleeritult kahte tulpa selliselt, et vasakul on esitatud lühend või mõiste ja paremal tulbas seletus. Inglisekeelsed sõnad seletustes esitatakse kaldkirjas. Alltoodud näited esitavad lühendite ja mõistete sõnastiku korrektset vormistamist.

%\begin{tabular}{p{3 cm}ll}
%IPv6&Internet Protocol version 6\\
%ICMPv6&Internet Control Message Protocol version 6\\
%Node&ll\\
%NAT&dd\\
%IANA&Internet Assigned Numbers Authority\\
%BYID&Bring Your Own Device\\
%OS&Operating System\\
%IoT&Internet of Things\\
%rootkit&ff
%\end{tabular}
%\pagebreak

\tableofcontents
\newpage
\listoffigures
\pagebreak
\listoftables
\pagebreak

\section{Introduction}
\label{sec:1}

Describes the problem statement, illustrates why this is a problem and describes the contribution the thesis makes in solving this problem. Optionally, 
it can give a short description (1-3 sentences each) of the remaining chapters. Good introductions are concise, typically no longer than 4 pages.\\
%The introduction reveals the full (but summarized) results of your work. This appears counter-intuitive: does this not break the tension, 
%like revealing the name of the murderer on the first page of a thriller? Yes, it does. That is the whole point. A thesis, and thus its architecture, 
%aims primarily to inform, not entertain.

\pagebreak
%---------------------------------TERMS AND DEFINITION -----------------------------------------
\section{Terms and definition}
\label{sec:2}
Defines the fundamental concepts your thesis builds on. Your thesis implements a new type of parser generator and uses the term non-terminal symbol a lot? 
Here is where you define what you mean by it. The key to this chapter is to keep it very, very short. 
Whenever you can, don’t reinvent a description for an established concept, but reference a text book or paper instead.
\pagebreak
%------------------------------BACKGROUND AND RELATED WORK-----------------------------------
\section{Background and Related Work}
\label{sec:3}
%4 -5 pages 
\subsection{Data Deduplication}
\label{sub:Deduplication}
Cloud computing is an on-demand service. Customers are charged based on used storage and bandwidth.\footnote{"With Amazon S3, you pay only for the storage you actually use. There is no minimum fee and no setup cost. Amazon S3 has three pricing components: storage (per GB per month), data transfer in or out (per GB per month), and requests (per n thousand requests per month)." \url{http://aws.amazon.com/s3/pricing/}} Both service providers and customers are interested in cost efficient solutions of cloud storage. Data deduplication offers disk and bandwidth savings. Idea is simple -- avoid or remove a duplicated data. 
This section covers basic concepts of deduplication technology. Lists various methods and processing type sand underlines the approaches used in cloud storage.

\subsubsection{Hash Based Deduplication}
\label{subsub:HashBased}
To remove or avoid duplicated data, it must be detected first. Hash based data deduplication uses the hash values of a file (or data chunk) as a file (or data chunk) identifier. Hashes of files are calculated and then are kept on the server. When the file is uploaded first time, its hash is computed and it is compared with the existing hashes on the server. If there is a match, the file is not stored on the disk (or in case of client-side deduplication, is not transfered at all). Instead, server creates the reference, which points on the already existing file, with the same hash value. If computed hash does not match with any of the hashes, the file together with the hash value is stored on the server.\cite{DeDupOverView}

\subsubsection{Types of Deduplication}
\label{subsub:TypesOfDedup}
Data deduplication differs based on processing methods. If it takes place before the client application transfers the file to the server, it is known as client-side deduplication. If it takes place, after the file is uploaded on server, it is known as server side deduplication. In client-side deduplication scenario, the client application computes the hash of the file and sends it to the server. If the hash already exists on the server side, client application does not send the file. If no match is found, it means,that the file is unique and client application transfers it to the server. On the other hand, if client application directly sends file to the server and server computes the hash after it, it is called server-side deduplication. Both processing methods save storage, but client-side deduplication also reduces bandwidth consumption.\cite{DeDupMethods}\\\\
Apart of divers processing methods, data deduplication differs in processing levels. There are file  and block level data deduplication. Difference between them is intuitive. In case of file level, hash of file is calculated and as a result server stores unique files. In block level scenario, files are divided into blocks(fixed or variable size). Hashes of these blocks are calculated and duplicated data on block level is avoided. \cite{DeDupMethods}\\\\
The last concept is, single and cross client data deduplication. Single client data deduplication removes duplicated data  in scope of one user. Duplicated data will be stored on the server, if it belongs to different users. On the other hand, cross client deduplication vanishes the user boundaries and unique data would be shared among the users.\cite{DeDupMethods}

\subsubsection{Summary}
\label{subsub:SummaryDedup}
Cloud storage providers are looking for, the most efficient way to reduce the cost. In cross user client-side deduplication  case, file or ”chunks” of file are stored only ones on the disk and users are sharing the data. It reduces the bandwidth cost dramatically, because the deduplication takes palace on client side, and duplicated files are not uploaded at all.\cite{DeDupDropBox} Such cost reductions is attractive for cloud storage providers, but this technology has some security drawbacks. \hyperref[sub:ConfidentialityAndPrivacy]{Next section} covers potential attacks taking place during cross-user client-side data deduplication in cloud storage.


\subsection{Confidentiality and Privacy Issues in Remote Storage}
\label{sub:ConfidentialityAndPrivacy}
%$f$
Although deduplication is a beneficial technology, there are security drawbacks, leading to potential attacks. Taking into consideration the behavior of the cross-user client-side data deduplication, it is easy to learn some general facts. This section focuses on attacks breaching the confidentiality of data and privacy of remote storage customers, when cross-user client-side data deduplication takes place.

\subsubsection{Potential Attacks}
\label{subsub:TypesOfDedup}
Danny Harnik was first who has demonstrated, the potential attacks in remote storage related to data deduplication technology.\cite{Harnik} The paper covers three cases: file detection, file content detection and covert channel. The first case shows, how  trivial is  to learn whether the remote server already contains the particular file or not. Attacker uploads the file and observes the network traffic or the time required to upload the file. If the file already is stored on the server, there is no need to upload it again. Client application sends only the 
hash of the file to the server. The observer detects, that amount of data is smaller then file's size itself(Size of the hash depends on hash function and is smaller then file size). If file is "big enough", observing the time required to upload file on server, is sufficient to learn, whether the server already contains the file or not. The law enforcement authorities, can use this behavior. Check if storage provider contains the file (e.g. file's content is against the law) and later, they can force remote storage service providers to revile the identity of the file owner.\\\\
Data deduplication technology opens the possibility to guess the content of user's data.The approach is straightforward, attacker just tries all possible variations and waits for occurrence of data deduplication. Once it takes place, attacker learns that such file (file with this content) exist on the server. The trick is that, unlike the dictionary attacks it is not detectable. It is the legitimate way to upload new documents on the server.\cite{Harnik} This type of attack is easy to lunch against the files with small min-entropy. To have batter understanding, refer to the following example. Bob is invited at the event in the cinema. He stores his invitation ticket in the cloud. Alice wants to learn the row and the place of Bob's ticket. She put the Bob's name on the right place and  starts to brute force row and place numbers. Alice generates files with different content and uploads on cloud. Once the deduplication takes place, she will get the desired information.\\\\
Last case describes the covert channel. Precondition for this scenario is, that attacker already have to own the victims machine. In order to exchange one bit information "0" or "1", attacker generates two random files  and uploads one of them. If the first file is uploaded the covert channel transfers "0" else it transfers "1" bit. Covert channel can transfers more information, by altering  the number of files or the meaning of file.\cite{Harnik}\\\\
All above stated attacks demonstrates the side channel effects of data deduplication. Attackers exploit the vulnerability, that data deduplication is detectable. But later Halevi states that main issue is not the detectability, but using the hash value as a proxy in remote storage.\cite{PoW} He claims that, to use a hash as a proxy to retrieve the file is vulnerable. Owning a small static piece of the file(e.g. hash of the file) does not necessarily mean owning the entire file. He referees to the Dropship\footnote{\url{https://github.com/driverdan/dropship} - "Instantly transfer files between Dropbox accounts using only their hashes"} open source project, as a brief example of misusing the storage provider. Dropship turn the remote storage  provider into CDN (Content Distribution Network) service. For that time Dropbox\footnote{\url{https://www.dropbox.com/}} was operating based on the cross-user client-side deduplication. The users of Dropship, where able to download the file in their folder, just sending the file’s hash for check to the Server. This open source project was considered as the violation of Terms of Service of the company and is not operating anymore. Halevi introduces the Proof of Ownership Protocol, which dramatically reduces the probability of the attacker to retrieve the file, without owning it. \hyperref[sec:4]{Next section} covers the detail description of Proof of Ownership Protocol and other solutions offered to substitute the hash as a proxy approach for data ownership in remote storage.\cite{PoW}
%Collects descriptions of existing work that is related to your work. Related, in this sense, means aims to solve the same problem or uses the same approach to solve a different problem. 
%This chapter typically reads like a structured list. Each list item summarizes a piece of work (typically a research paper) briefly and explains the relation to your work. 
%This last part is absolutely crucial: the reader should not have to figure out the relation himself.
%Is your piece better from some perspective? More generalizable? More performant? Simpler? It is ok if it is not, but I want you to tell me.

\subsection{Summary}
\label{subsub:AttackSummary}
The amount of savings offered by data deduplication, depends on data type and content produced by users of such services.\cite{ratio}  In case of office workers as users in remote storage, the benefit from deduplication is high. Office workers use mostly identical template to generate the data and the portion of duplication is high. Applying data deduplication technology saves bandwidth and disk space.  But same time it rises privacy and confidentiality issues. The major weakness is that, client-side deduplication is detectable and using hash as a proof of ownership is not sufficient. Anyone who possesses the hash value of file, is able to retrieve the file from the server.  If the attacker  obtains the hash of the file, he can retrieve the file from the server and gain unauthorized access to it.\\\\
Data privacy issue in cloud computing is one of the aspects that could break the trust of the users towards the service providers. So those who what to stay on the market, should build the systems, which takes into consideration privacy and confidentiality.

\pagebreak

%------------------------------APPROACH-----------------------------------
\section{Approach}
\label{sec:4}

We have demonstrated importance of data deduplication technology for remote storage services. And have determine the root  cause of breaching the privacy and confidentiality. This section covers the solutions, which refuse to use the static piece of information (hash of the file) as a proxy and offers alternative ways to proof the ownership of the data. We numerate the solutions from one to seven based on published date and show how it works and what are there security and efficiency characteristics. \\\\
\subsection{Solution \# 1}
\label{sub:Soltuion1}

This subsections covers Proof of Ownership (PoW) protocol, introduced by Halevi.\cite{PoW}PoW involves two parties: Prover and Verifier. The goal of prover is to convince the verifier, that he "owns" particular file. While the goal of verifier is to check if the affirmation of the prover is true. To accomplish their tasks, verifier uses summary value of file, while  prover relies on  the file itself. Paper \cite{PoW} offers three solutions, and the subsection reviews all of them, but covers security and efficiency characteristics only  for the last one. Before we move to the solutions, we have to underline two constraints. First, attacker may have compliances which own the file, but the total number of bits that attacker can receive from them must be less then initial min-entropy\footnote{"The min entropy, in information theory, is the smallest of the Rényi family of entropies, corresponding to the most conservative way of measuring the unpredictability of a set of outcomes, as the negative logarithm of the probability of the most likely outcome." "A random variable $X$ has \textbf{min-entropy} $k$, denoted $H_\propto(X)=k$, if  $\underset{x}{max}Pr[X=x]=2^{-k}$  "\cite{MinEnt} } of file. And second, attacker can not interact with compliances during the proving phase.( e.g. case misusing the remote storage as CDN)
\subsubsection{Setup}
\label{subsub:setup1}
The most secure and less efficient  from suggested three solutions, uses erasure code.\footnote{"The basic premise of erasure coding goes as follows: Take a file and split into k pieces and
encode into n pieces. Now, any k pieces can be used to get back the file”} Form each 90\% of bits, it is possible to recover the whole file. After the file is encoded using erasure code, next step is to build the Merkle-tree\cite{Merkle} on the encoded file. The verifier(server) keeps the root value of the computed tree and the number of leaves. During the poof phase, verifier(server) asks the  prover(client)  for some number of leaves' values and their sibling paths. The verifier checks if all the provided sibling paths gives the valid Merkle-tree root value. Based on the outcome, server grants or does not grant the access to the file.\\\\
Computing erasure code requires access to the file and in case of large files (the files stored on the secondary storage) it raises communication complexity. To increase the efficiency of the protocol, erasure encoding is substituted with universal hashing\cite{Hash}. First the file is hashed and then the Merkle-tree is built on the hash. The hashing servers to reduce the size of file up to some predefined number of bits(max length 64MByte). The second solution is more efficient then first one, but it  weakens the security. Security requirement for first solution claims: attacker can not retrieve file from the server, if the min-entropy remained in file after attacker receives the bits form compliances, is bigger then security parameter. Erasure encoding  substitution with universal hashing, made changes in security requirement as well. For second solution, security requirement stress that, attacker can convince the verifier to grant access to the file, if it receives some $T$ bits from compliances, which can be less then min-entropy of the file. (e.g.64MByte)\\\\
Erasure code and universal hashing solutions, both considers that the input file is taken form an arbitrary distribution. On the other hand, the third solution claims that, in realistic scenarios, the attacker always has some information about file which he desires to extract.Thereof, it is resalable to relax the security requirement and define  it for particular class of distribution. Such relaxation of security requirement gives possibility to modify the protocol and make it more space efficient.  In particular instead of working with bit vectors, it is possible to divide file into blocks and operate over the blocks. There are three phases to prepare the input for Merkle-Tree: Initializing, reducing and mixing. First the $M$ bit size file is divided into $m$ blocks. In the initializing phase, $l$ blocks of buffer and IV (Initial Vector) are allocated. Next comes reduction phase, which is a liner mapping. It maps, original file's $m$ blocks to the allocated $l$ buffer blocks. Each block of the file is XORed in specific number in some locations. And locations are taken from IV, which is generated as SHA256(IV[i-1],File[i]). Where $i$ is the block number of the file and IV[0] is defined as SHA256-IV.\footnote{For SHA-256, the initial hash value, H(0), consists of the eight 32-bit words, in hex. These words were obtained by taking the first thirty-two bits of the fractional parts of the square roots of the first eight prime numbers.\url{https://tools.ietf.org/html/rfc4634\#section-6.2}}. The same operations take palace at mixing phase. But with one difference, instead of file blocks, buffer blocks are taken as input of XORing. 
\subsubsection{Security And Efficiency}
\label{subsub:secAndeff1}


To demonstrate the soundness of the last solution, it is better to view the file from attacker's perspective. Input file in this scenarios is not take form arbitrary distribution, but form some class of distribution. And it reasonable for real life scenarios, as attacker always know some peace of information(e.g. file format) about the file that he tries to retrieve. $M$-bit file with $k$ bits of min-entropy, can be represented from attackers perspective as $\vec{f}\leftarrow\vec{w}\cdot A +\vec{b}$, where $\vec{w} \in \{0,1\}^k$  and is chosen randomly, while $A\in\{0,1\}^{k\times M}$ and $\vec{b}\in\{0,1\}^M$ are chosen by attacker(based some knowledge that attacker has). Protocol uses hash function to prepare input for Merkle-tree, which is linear mapping, $h(\vec{f})=\vec{f}\cdot C = \vec{w}\cdot AC + \vec{b}C$.\cite{PoW} Important part in this linear mapping is that the linear code that is generated by AC matrix to have a large minimum distance. And it is possible to achieve as we are choosing matrix C for mapping. The theorem \#3 proved in the paper states that the last solution is the secure proof of ownership with soundness $\left( \frac{L-d+1}{L}\right)^t$ where $L$ is reduce buffer, $t$ is number of challenges on Markle-tree and $d$ is the minimum distance of the linear code generated by AC matrix.("For example, if the code has minimum distance$\geqslant \frac{l}{3}$ then we get soundness of ($\frac{2}{3}$)$^t$ .")\cite{PoW}\\\\
Time efficiency is one of the important features, that characterizes the protocol and influences decision weather to implement it or not. Halevi evaluates the performance of PoW protocol, and  compares it with non-secure  \hyperref [sub:Deduplication]{data-deduplication} and whole file transfer (without data-deduplication) implementations of remote storage. Overall time protocol requires, is decomposed in three parts: Client, Server and Network time.\footnote{"The measurements were performed on an Intel machine with Xeon X5570 CPU running at 2.93GHz. We implemented the protocol in C++ and used the SHA256 implementation from Crypto++"} Client time is calculated as the sum of the subtasks client performs and subtasks are: reading file from the disk, computing the SHA256 hash, going through reduction and mixing phases and  computing the Merkle-tree. Server time -- the time server needs to check Mekle-tree authentication signature. And Network time -- respectively the time necessary for data generated by prover and verifier to travel via network. Server and Network time consumption is negligible. (E.e. checking 20 sibling paths "costs" 0.6ms and data generated for transmission  is less then 20K. In case of 5Mbps network the overhead is 0.1 ms. All together the overhead of Server and Network is 0.7 ms). While the main pressure comes on client side. To  compare it with insecure implementation of \hyperref [sub:Deduplication]{data-deduplication}, PoW on client side adds reduction and mixing phases and Markle-tree calculation. As result of the tests, reduction phase adds less then 28\% time over insecure solution. Mixing phase and Mekle-tree calculation behavior depends on th size of the file. For small size files(less then 64MByte), the time up-growth is 200\% , but it stays  constant(1158ms) once the file grows above 64MByte. PoW is also  compare with the solutions to avoid deduplication and always send a whole file to the server. Protocol is observed in two setups: network with 5Mbps and 100Mbps. The results are following: PoW always consumes less time in 5Mbps then transferring the whole file. Once the file grows over the 1GByt PoW requires 1\%  time of the file transfer. In case of 100Mbps network, the protocol has lower bound for file size. For files larger then 64K, PoW consumes less time then solution without deduplication . And for files larger then 1GByt, it requires 4\% of time of the whole file transfer. 

\begin{savenotes}
\begin{table}[!htpb]
\centering
\addtolength{\tabcolsep}{3pt}
\begin{tabular}{|L{1cm}|L{4.5cm}|L{4.5cm}|}
\rowcolor{lightblue}
\hline
&Dedup Time  $= T_d$ &File Transfer Time $=T$ \\
\hline
PoW & $\color{red} 3.28T_d+0.7ms$\footnote{For files less then 64Mb: $T_d+0.28T_d+2T_d+0.7ms$}  and $\color{red} 1.28T_d+1.165s$\footnote{For files more then 64Mb: $T_d+0.28T_d+2T_d+0.7ms$ } 
& $\color{red} 0.1T$\footnote{In 5Mbps network and file size more then 1Gb}; $\color{red} 0.4T$ \footnote{In 100Mbps network and file size more then 1Gb}; 
$\color{red} > T$ \footnote{In 5Mpbs for any size of file and In 100Mbps for files larger then 64K}; \\
\hline

\end{tabular}
\caption{PoW 1Time Comparison}
\label{table:paramProbCodes}
\end{table}
\end{savenotes}



\subsection{Solution \# 2}
\label{sub:Soltuion2}

This subsection covers the solution proposed by Di Pietro\cite{DiPietro} in his paper "Boosting Efficiency and Security in Proof of Ownership for Deduplication". The motivation of this work is to improve the efficiency of PoW\cite{PoW} protocol and avoid the security  assumptions that is hard to verify(refereeing to the concept that the file is taken from some class of distribution and not from arbitrary distribution). The subsection includes the description how the scheme works and  efficiency analyses in comparison with PoW\cite{PoW} 

\subsubsection{Setup}
\label{subsub:setup2}


Solution offered by Di Pietro is two party protocol and involves $C$(Client) as a Prover  and $S$(Server) as a Verifier.He names a protocol as s-POW. Once $S$ receives the file for the first time it computes the $n$ number of challenges and stores the file on the disk. To compute the challenges $S$ keeps the hash-map data structure $\Im$. It maps strings to the tuples and as a key it uses the hash of the file. Tuple contains  4 elements: $ptr$ -- the pointer on the file; $res []$ -- an array of generated responses ($K$ bit string); $id_c$ -- the highest challenge computed so far ; $id_u$ -- number of challenges used so far.

\begin{figure}[ht] 
\begin{center}
\includegraphics[width=0.8\textwidth]{Di_Pietro_Hash_Map}
\caption{$\Im$ Hash-Map Structure}
\label{fig:Di_Pietro_Hash_Map} .
\end{center}
\end{figure}

The $S$ uses file digest $d$ (hash of the file), $id_c$ index and server's master key as an input for the  pseudo-random generator $F$ to produce random seed $s$. $s$ is an integer and $0 < s < file\_size $. The random seed $s$ serves for calculation of random position and is unique of each challenge. At the end based on the random  position and the file, $get-bit$ macros outputs the bit value. Concatenation of such outputs represents the responses, which is $K$ bits log and is stored in $res[]$ array. Server computes $n$ number of responses at a time. This approach reduces I/O operations. Responses computation takes place only, when the client uploads the file , which did not exist on server before or when  all the pre-computed responses are depleted.($id_c$ and $id_u$ control which responses is still valid and who many valid ones are remained.)\\\\
If client $C$ attempts to upload the file already located on server $S$, $S$ challenges the client, sends the random seed $s$ and waits for valid response. Client receives $s$ and use same $get-bit$ macro to produce a $K$ bit length response. If received response is the same as pre-computed one, then client succeeds to convince the server.\\\\
The s-POW protocol has two other modifications, which are designed to improve efficiency and are more convenient in particular cases. In one case the hash function which is used to calculate the key of the hash-map is substitute with calculation of response($K$ bit string) based on the file and the public seed $S_{pub}$. (This response servers as the key of hash-map) . In second case the file size is used as the key of the hash-map. The second solution is worthy only for large files, as the collision will be extremely high.

\subsubsection{Security And Efficiency}
\label{subsub:secAndeff2}

To demonstrate the security of s-POW protocol, Di Pietro shows the probability of adversary to convince the verifier and it is assumed that adversary already owns some large part of file. The Probability of adversary to guess the single bit for the $K$ bit length response is:$P(succ_1) = 1-\varepsilon(1-g)$, where $\varepsilon$ is fraction of file that is unknown for attacker and $g$ is probability to guess unknown bit correctly. To convince the server adversary should guess the whole $k$ bit length response, and as guessing each bit from K bit vector are independent events, the probability of convincing server is: $P(succ) = (1-\varepsilon(1-g))^K$. It means that the success probability  of adversary to convince the server depends on $K$, which is possible to tune based the security requirement. E.g. if the requirement is , $P(succ)\leq2^{-k}$, where $k$ is a security parameter, then $K =\lceil \frac{k ln 2}{\varepsilon(1-g)} \rceil $\\\\
Efficiency analyses is represented in the CPU computation, I/O and bandwidth terms in bought client and server side. Di Pietro evaluates his proposed schemes(s-POW and s-POW1 \footnote{Modification of s-POW protocol, which uses $K$ bit string  instead of file hash as a key in hash-map}) and compares it with PoW.\footnote{"We have run our implementation of both schemes on a 64-bit RedHat box with an Intel Xeon 2.27GHz CPU, 18 GiB of RAM and an IBM 42D0747 7200 RPM SATA hard disk drive."}\cite{PoW} On client-side both s-POW and s-POW1 schemes are faster then PoW. The complexity up growth of s-POW and PoW scheme are equivalent of file size growth. It is resalable because in both cases the dominant operation is hashing. While in s-POW1 computation cost became  constant for large files. As in s-POW1 no hashing is used and only random disk access is needed to get required bit. For  visualization it is better to refer the diagrams provided by authors:
\begin{figure}[ht] 
\begin{center}
\includegraphics[width=1.0\textwidth]{POWvsPoW}
\caption{Comparison of running time on client-side}
\label{fig:POWvsPoW} .
\end{center}
\end{figure}

To demonstrate the server-side performance, it is convenient to divide it in two phases: initialization and regular execution. When the file is first uploaded on server-side, that represent initialization phase, all other communication between client and server is the regular execution phase. In initialization phase PoW and s-POW both performs file hashing. It follows with reduction and mixing phase and Merkle-tree calculation in case of  \hyperref[sub:Soltuion1]{PoW} and $n$ challenge computation in s-POW case.In s-POW when all pre-calculated challenges are used, server should pre-calculate them again but this is considered to be a part of regular execution phase. In regular execution phase in PoW performs Merkle-tree verification while in s-POW it performs only look-up to get the correct tuple and the valid response from $res[]$  array. But in addition to look-up, s-POW needs to recalculate the challenges(responses). Dispite the pre-calculated challenges, which are done in order to decrees the file access, on server side PoW is faster then provided s-POW. It is also importatn to mention the storage efficiency, PoW need only Merkle-tree root value to store on server side, while s-POW for operation need storing the hash-map data structure.  Base on the performed analyses and execution, authors gives the asymptotic analyses of schemes:


\begin{savenotes}
\begin{table}[!htpb]
\centering
\addtolength{\tabcolsep}{3pt}
\begin{tabular}{|L{4cm}|L{2.5cm}|L{2.5cm}|L{2.5cm}|}
\rowcolor{lightblue}
\hline
&PoW&s-POW&s-POW1\\
\hline
Client-side computation&$O(m)hash$ & $O(m)hash$ & $O(k)PRNF\footnote{Pseudorandom number generator}$ 
Client-side I/O&$O(m)$&$O(m)$&$O(k)$
Server-side computation (initialization phase)&&&
Server-side computation (regular execution phase)&&&
Server-side I/O (initialization phase)&&&
Server-side I/O (regular execution phase)&&&
Server-side storage&&&
Bandwidth&&&

\end{tabular}
\caption{PoW 1Time Comparison}
\label{table:paramProbCodes}
\end{table}
\end{savenotes}



 
%Outlines the main thing your thesis does. Your thesis describes a novel algorith for X? Your main contribution is a case study that replicates Y? Describe it here.
\begin{itemize}
 \item Proofs of Ownership in Remote Storage Systems - 2011 + 
 \item Boosting Efficiency and Security in Proof of Ownership for Deduplication - 2012 - 
 \item Provable Ownership of File in De-duplication Cloud Storage - 2013
 \item Leakage-Resilient Client-side Deduplication of Encrypted Data in Cloud Storage - 2013
 \item A Secure Client Side Deduplication Scheme in Cloud Storage Environments - 2014
 \item A Tunable Proof of Ownership Scheme for Deduplication Using Bloom Filters - 2014
 \item An efficient confidentiality-preserving Proof of Ownership for Deduplication -2014
 
\end{itemize}





\pagebreak

%------------------------------EVALUATION-----------------------------------
\section{Evaluation}
\label{sec:5}
Demonstrate why the developed framework  is secure and efficient. 


\begin{enumerate}
 \item Time complexity evaluation.
 \item Cost analyses. 
\end{enumerate}



%Describes why your approach really solves the problem it claims to solve. You implemented a novel algorithm for X? 
%This chapter describes how you ran it on a dataset and reports the results you measured. You replicated a study? This chapter gives the results and your interpretations.

%The Appraoch and Evaluation chapters contain the meat of your thesis. Often, they make up half or more of the pages of the entire document.

\pagebreak

%------------------------------FUTURE WORK-----------------------------------
\section{Future Work}
\label{sec:6}

 In science folklore, the merit of a research question is compounded by the number of interesting follow-up research questions it raises. 
 So to show the merit of the problem you worked on, you list these questions here.
 %If you don’t care about research folklore (I did not as a student),
 %this chapter is still useful: whenever you stumble across something that you should do if you had unlimited time, but cannot do since you don’t, you describe it here. 
 %Typical candidates are evaluation on more study objects, investigation of potential threats to validity, … 
 %The point here is to inform the reader (and your supervisor) that you were aware of these limitatons. Limit this chapter to very few pages. 
 %Two is entirely fine, even for a Master’s thesis.
\pagebreak


%------------------------------CONCLUSIONS-----------------------------------
\section{Conclusions}
\label{sec:7}
Short summary of the contribution and its implications. The goal is to drive home the result of your thesis.
Do not repeat all the stuff you have written in other parts of the thesis in detail. Again, limit this chapter to very few pages. 
The shorter, the easier it is to keep consistent with the parts it summarizes.

\pagebreak

%------------------------------REFERENCES-----------------------------------

\addcontentsline{toc}{section}{References}


\bibliographystyle{IEEEtran}
\bibliography{bibi.bib}


\pagebreak

%-----------------------------APPENDIX--------------------------------

\appendix

\section{Appendix 1}




\end{document}
